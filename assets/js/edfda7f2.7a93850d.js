"use strict";(self.webpackChunkswat_hub=self.webpackChunkswat_hub||[]).push([[7290],{15680:(e,n,a)=>{a.d(n,{xA:()=>p,yg:()=>f});var t=a(96540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function i(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function s(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=t.createContext({}),c=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):i(i({},n),e)),a},p=function(e){var n=c(e.components);return t.createElement(l.Provider,{value:n},e.children)},m="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},u=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(a),u=r,f=m["".concat(l,".").concat(u)]||m[u]||g[u]||o;return a?t.createElement(f,i(i({ref:n},p),{},{components:a})):t.createElement(f,i({ref:n},p))}));function f(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=u;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return t.createElement.apply(null,i)}return t.createElement.apply(null,a)}u.displayName="MDXCreateElement"},24931:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var t=a(58168),r=(a(96540),a(15680));const o={title:"Configuring self-monitoring",description:"How to configure self-monitoring of IBM CloudPak for AIOps with OpenShift",sidebar_position:30},i=void 0,s={unversionedId:"practitioner-basics/monitoring/configuring-self-monitoring",id:"practitioner-basics/monitoring/configuring-self-monitoring",title:"Configuring self-monitoring",description:"How to configure self-monitoring of IBM CloudPak for AIOps with OpenShift",source:"@site/labs/practitioner-basics/4-monitoring/configuring-self-monitoring.mdx",sourceDirName:"practitioner-basics/4-monitoring",slug:"/practitioner-basics/monitoring/configuring-self-monitoring",permalink:"/waiops-tech-jam/labs/practitioner-basics/monitoring/configuring-self-monitoring",draft:!1,editUrl:"https://github.com/IBM/waiops-tech-jam/tree/main/labs/practitioner-basics/4-monitoring/configuring-self-monitoring.mdx",tags:[],version:"current",sidebarPosition:30,frontMatter:{title:"Configuring self-monitoring",description:"How to configure self-monitoring of IBM CloudPak for AIOps with OpenShift",sidebar_position:30},sidebar:"tutorialSidebar",previous:{title:"Reclaiming Free Space",permalink:"/waiops-tech-jam/labs/practitioner-basics/storage/data-foundation/maintenance"},next:{title:"Labs Overview",permalink:"/waiops-tech-jam/labs/instana/introduction/"}},l={},c=[{value:"Examining KPIs and other metrics using User Workload Monitoring",id:"examining-kpis-and-other-metrics-using-user-workload-monitoring",level:2},{value:"Enabling user workload monitoring",id:"enabling-user-workload-monitoring",level:3},{value:"Viewing metrics in the OCP console",id:"viewing-metrics-in-the-ocp-console",level:3},{value:"Enabling additional metrics",id:"enabling-additional-metrics",level:3},{value:"Kafka",id:"kafka",level:4},{value:"Viewing metrics in Grafana",id:"viewing-metrics-in-grafana",level:3},{value:"Installation",id:"installation",level:4},{value:"Creating dashboards",id:"creating-dashboards",level:4}],p={toc:c},m="wrapper";function g(e){let{components:n,...a}=e;return(0,r.yg)(m,(0,t.A)({},p,a,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"This page covers some strategies for observing IBM CloudPak for AIOps KPIs and how to\nconfigure them."),(0,r.yg)("h2",{id:"examining-kpis-and-other-metrics-using-user-workload-monitoring"},"Examining KPIs and other metrics using User Workload Monitoring"),(0,r.yg)("p",null,"OpenShift comes with a prometheus-based monitoring stack, which out of the box is used to\ntrack metrics about the cluster."),(0,r.yg)("p",null,"It is possible to extend this monitoring to 'user workloads' (i.e. anything that is not a\ncore OCP service), by ",(0,r.yg)("a",{parentName:"p",href:"https://docs.openshift.com/container-platform/4.15/observability/monitoring/enabling-monitoring-for-user-defined-projects.html"},"enabling user workload monitoring"),". This will deploy a secondary prometheus instance, which can be queried alongside\nthe cluster-level prometheus, as if it was one instance, using ",(0,r.yg)("a",{parentName:"p",href:"https://thanos.io"},"thanos"),"."),(0,r.yg)("p",null,"There are various services in AIOps which are capable of exposing metrics to prometheus\nin this way."),(0,r.yg)("h3",{id:"enabling-user-workload-monitoring"},"Enabling user workload monitoring"),(0,r.yg)("p",null,"The first step is to enable the capability, if it has not been already. This is a\ncluster-wide setting, so care should be taken if this is being applied to a shared cluseter."),(0,r.yg)("p",null,"It can be enabled by running the following commands when logged into the cluster:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"oc create -n openshift-monitoring -f - <<EOF || true\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/created-by: vision-deploy-tool\n  name: cluster-monitoring-config\n  namespace: openshift-monitoring\ndata:\n  config.yaml: |\n    enableUserWorkload: true\nEOF\n\noc create -n openshift-user-workload-monitoring -f - <<EOF || true\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/created-by: vision-deploy-tool\n  name: user-workload-monitoring-config\n  namespace: openshift-user-workload-monitoring\ndata:\n  config.yaml: |\nEOF\n")),(0,r.yg)("p",null,"More advanced configuration is possible; refer to the\n",(0,r.yg)("a",{parentName:"p",href:"https://docs.openshift.com/container-platform/4.15/observability/monitoring/enabling-monitoring-for-user-defined-projects.html"},"OpenShift documentation")," for more details."),(0,r.yg)("h3",{id:"viewing-metrics-in-the-ocp-console"},"Viewing metrics in the OCP console"),(0,r.yg)("p",null,"At this point, some AIOps metrics will become available in the OCP console under\n",(0,r.yg)("inlineCode",{parentName:"p"},"Observe -> Metrics"),". This will allow you to enter any\n",(0,r.yg)("a",{parentName:"p",href:"https://prometheus.io/docs/prometheus/latest/querying/basics/"},"prometheus query")," and graph the result."),(0,r.yg)("p",null,"Below are some example queries for AIOps metrics:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Rate of events ingested by lifecycle over time:",(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},'sum(irate((flink_taskmanager_job_task_operator_KafkaSourceReader_topic_partition_currentOffset{topic="cp4waiops_cartridge_lifecycle_input_events"}>0)[1m:30s]))\n'))),(0,r.yg)("li",{parentName:"ol"},"Rate of alert update messages processed by lifecycle over time:",(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},'sum(irate((flink_taskmanager_job_task_operator_KafkaSourceReader_topic_partition_currentOffset{topic="cp4waiops_cartridge_lifecycle_input_alerts"}>0)[1m:30s]))\n'))),(0,r.yg)("li",{parentName:"ol"},"Average kafka message size for messages produced by lifecycle:",(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"sum(flink_taskmanager_job_task_operator_KafkaProducer_record_size_avg > 0) by (operator_name)\n"))),(0,r.yg)("li",{parentName:"ol"},"Lifecycle kafka consumer lag (i.e. how far behind the latest message it is):",(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"sum by (task_name, subtask_index) (flink_taskmanager_job_task_operator_pendingRecords)\n")))),(0,r.yg)("h3",{id:"enabling-additional-metrics"},"Enabling additional metrics"),(0,r.yg)("p",null,"While some components enable metric scraping out of the box (e.g. lifecycle, as in the examples\nabove), others need additional configuration."),(0,r.yg)("h4",{id:"kafka"},"Kafka"),(0,r.yg)("p",null,"A large portion of the communication in AIOps happens over Kafka, so scraping metrics from this\nis highly valuable."),(0,r.yg)("p",null,"The kafka prometheus exporter can be enabled and configured by running the following commands in the\nAIOps namespace:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'KAFKA_METRICS_CONFIGMAP=\'\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kafka-metrics\n  labels:\n    app: strimzi\ndata:\n  kafka-metrics-config.yml: |\n    # See https://github.com/prometheus/jmx_exporter for more info about JMX Prometheus Exporter metrics\n    lowercaseOutputName: true\n    rules:\n    # Special cases and very specific rules\n    - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n      name: kafka_server_$1_$2\n      type: GAUGE\n      labels:\n       clientId: "$3"\n       topic: "$4"\n       partition: "$5"\n    - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><>Value\n      name: kafka_server_$1_$2\n      type: GAUGE\n      labels:\n       clientId: "$3"\n       broker: "$4:$5"\n    - pattern: kafka.server<type=(.+), cipher=(.+), protocol=(.+), listener=(.+), networkProcessor=(.+)><>connections\n      name: kafka_server_$1_connections_tls_info\n      type: GAUGE\n      labels:\n        cipher: "$2"\n        protocol: "$3"\n        listener: "$4"\n        networkProcessor: "$5"\n    - pattern: kafka.server<type=(.+), clientSoftwareName=(.+), clientSoftwareVersion=(.+), listener=(.+), networkProcessor=(.+)><>connections\n      name: kafka_server_$1_connections_software\n      type: GAUGE\n      labels:\n        clientSoftwareName: "$2"\n        clientSoftwareVersion: "$3"\n        listener: "$4"\n        networkProcessor: "$5"\n    - pattern: "kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+):"\n      name: kafka_server_$1_$4\n      type: GAUGE\n      labels:\n       listener: "$2"\n       networkProcessor: "$3"\n    - pattern: kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+)\n      name: kafka_server_$1_$4\n      type: GAUGE\n      labels:\n       listener: "$2"\n       networkProcessor: "$3"\n    # Some percent metrics use MeanRate attribute\n    # Ex) kafka.server<type=(KafkaRequestHandlerPool), name=(RequestHandlerAvgIdlePercent)><>MeanRate\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)Percent\\w*><>MeanRate\n      name: kafka_$1_$2_$3_percent\n      type: GAUGE\n    # Generic gauges for percents\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)Percent\\w*><>Value\n      name: kafka_$1_$2_$3_percent\n      type: GAUGE\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)Percent\\w*, (.+)=(.+)><>Value\n      name: kafka_$1_$2_$3_percent\n      type: GAUGE\n      labels:\n        "$4": "$5"\n    # Generic per-second counters with 0-2 key/value pairs\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)PerSec\\w*, (.+)=(.+), (.+)=(.+)><>Count\n      name: kafka_$1_$2_$3_total\n      type: COUNTER\n      labels:\n        "$4": "$5"\n        "$6": "$7"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)PerSec\\w*, (.+)=(.+)><>Count\n      name: kafka_$1_$2_$3_total\n      type: COUNTER\n      labels:\n        "$4": "$5"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)PerSec\\w*><>Count\n      name: kafka_$1_$2_$3_total\n      type: COUNTER\n    # Generic gauges with 0-2 key/value pairs\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Value\n      name: kafka_$1_$2_$3\n      type: GAUGE\n      labels:\n        "$4": "$5"\n        "$6": "$7"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+), (.+)=(.+)><>Value\n      name: kafka_$1_$2_$3\n      type: GAUGE\n      labels:\n        "$4": "$5"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)><>Value\n      name: kafka_$1_$2_$3\n      type: GAUGE\n    # Emulate Prometheus \'Summary\' metrics for the exported \'Histogram\'s.\n    # Note that these are missing the \'_sum\' metric!\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Count\n      name: kafka_$1_$2_$3_count\n      type: COUNTER\n      labels:\n        "$4": "$5"\n        "$6": "$7"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)><>(\\d+)thPercentile\n      name: kafka_$1_$2_$3\n      type: GAUGE\n      labels:\n        "$4": "$5"\n        "$6": "$7"\n        quantile: "0.$8"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+), (.+)=(.+)><>Count\n      name: kafka_$1_$2_$3_count\n      type: COUNTER\n      labels:\n        "$4": "$5"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+), (.+)=(.*)><>(\\d+)thPercentile\n      name: kafka_$1_$2_$3\n      type: GAUGE\n      labels:\n        "$4": "$5"\n        quantile: "0.$6"\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)><>Count\n      name: kafka_$1_$2_$3_count\n      type: COUNTER\n    - pattern: kafka.(\\w+)<type=(.+), name=(.+)><>(\\d+)thPercentile\n      name: kafka_$1_$2_$3\n      type: GAUGE\n      labels:\n        quantile: "0.$4"\n  zookeeper-metrics-config.yml: |\n    # See https://github.com/prometheus/jmx_exporter for more info about JMX Prometheus Exporter metrics\n    lowercaseOutputName: true\n    rules:\n    # replicated Zookeeper\n    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\\\d+)><>(\\\\w+)"\n      name: "zookeeper_$2"\n      type: GAUGE\n    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\\\d+), name1=replica.(\\\\d+)><>(\\\\w+)"\n      name: "zookeeper_$3"\n      type: GAUGE\n      labels:\n        replicaId: "$2"\n    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\\\d+), name1=replica.(\\\\d+), name2=(\\\\w+)><>(Packets\\\\w+)"\n      name: "zookeeper_$4"\n      type: COUNTER\n      labels:\n        replicaId: "$2"\n        memberType: "$3"\n    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\\\d+), name1=replica.(\\\\d+), name2=(\\\\w+)><>(\\\\w+)"\n      name: "zookeeper_$4"\n      type: GAUGE\n      labels:\n        replicaId: "$2"\n        memberType: "$3"\n    - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\\\d+), name1=replica.(\\\\d+), name2=(\\\\w+), name3=(\\\\w+)><>(\\\\w+)"\n      name: "zookeeper_$4_$5"\n      type: GAUGE\n      labels:\n        replicaId: "$2"\n        memberType: "$3"\n\'\n\nKAFKA_PODMONITOR=\'\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: kafka-resources-metrics\n  labels:\n    app: strimzi\nspec:\n  selector:\n    matchExpressions:\n      - key: "ibmevents.ibm.com/kind"\n        operator: In\n        values: ["Kafka", "KafkaConnect", "KafkaMirrorMaker", "KafkaMirrorMaker2"]\n  namespaceSelector:\n    matchNames:\n      - myproject\n  podMetricsEndpoints:\n  - path: /metrics\n    port: tcp-prometheus\n    relabelings:\n    - separator: ;\n      regex: __meta_kubernetes_pod_label_(ibmevents_ibm_com_.+)\n      replacement: $1\n      action: labelmap\n    - sourceLabels: [__meta_kubernetes_namespace]\n      separator: ;\n      regex: (.*)\n      targetLabel: namespace\n      replacement: $1\n      action: replace\n    - sourceLabels: [__meta_kubernetes_pod_name]\n      separator: ;\n      regex: (.*)\n      targetLabel: kubernetes_pod_name\n      replacement: $1\n      action: replace\n    - sourceLabels: [__meta_kubernetes_pod_node_name]\n      separator: ;\n      regex: (.*)\n      targetLabel: node_name\n      replacement: $1\n      action: replace\n    - sourceLabels: [__meta_kubernetes_pod_host_ip]\n      separator: ;\n      regex: (.*)\n      targetLabel: node_ip\n      replacement: $1\n      action: replace\n\'\n\noc apply -f - <<EOF\n${KAFKA_METRICS_CONFIGMAP}\n---\n${KAFKA_PODMONITOR}\nEOF\n\noc patch kafka iaf-system --type=merge -p \'{\n  "spec": {\n    "kafka": {\n      "metricsConfig": {\n        "type": "jmxPrometheusExporter",\n        "valueFrom": {\n          "configMapKeyRef": {\n            "key": "kafka-metrics-config.yml",\n            "name": "kafka-metrics"\n          }\n        }\n      }\n    }\n  }\n}\'\n')),(0,r.yg)("p",null,"One these commands are run, the kafka brokers will perform a rolling restart in order to enable the metric exporter."),(0,r.yg)("p",null,"You will then be able to explore kafka broker metrics in the openshift console, for example:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Rate of messages published per kafka topic:",(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},'sum by (topic) (irate(kafka_server_brokertopicmetrics_messagesin_total{topic!=""}[1m]))\n')))),(0,r.yg)("h3",{id:"viewing-metrics-in-grafana"},"Viewing metrics in Grafana"),(0,r.yg)("p",null,"Whilst all metrics can be explored in the OCP console, it is fairly limited when it comes to visualization types\nand there is no way to construct and save a dashboard of multiple charts."),(0,r.yg)("p",null,"It is possible to install Grafana into OpenShift and configure it to connect to Thanos. This will give it access to\nall metrics across all of the prometheus instances (both cluster and user workload metrics)."),(0,r.yg)("h4",{id:"installation"},"Installation"),(0,r.yg)("p",null,"As a quick start, this can be installed with the following script:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-diff"},"@@ NOTE: This will not work for airgap deployments as it relies on pulling grafana from public registries @@\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'#!/bin/bash\n\noc create -f - <<EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring-grafana\n---\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: operatorgroup\n  namespace: monitoring-grafana\nspec:\n  targetNamespaces:\n  - monitoring-grafana\n---\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: monitoring-grafana\n  namespace: monitoring-grafana\nspec:\n  channel: v4\n  installPlanApproval: Automatic\n  name: grafana-operator\n  source: community-operators\n  sourceNamespace: openshift-marketplace\nEOF\n\noc project monitoring-grafana\n\ninstallReady=false\nwhile [ "$installReady" != true ]\ndo\n    installPlan=`oc get subscription.operators.coreos.com monitoring-grafana -n monitoring-grafana -o json | jq -r .status.installplan.name`\n    if [ -z "$installPlan" ]\n    then\n        installReady=false\n    else\n        installReady=`oc get installplan -n monitoring-grafana "$installPlan" -o json | jq -r \'.status|.phase == "Complete"\'`\n    fi\n\n    if [ "$installReady" != true ]\n    then\n        sleep 5\n    fi\ndone\ninstallReady=false\nwhile [ "$installReady" != true ]\ndo\n    csv=`oc get subscription.operators.coreos.com monitoring-grafana -n monitoring-grafana -o json | jq -r .status.currentCSV`\n    if [ -z "$csv" ]\n    then\n        installReady=false\n    else\n        installReady=`oc get csv -n monitoring-grafana "$csv" -o json | jq -r \'.status.phase == "Succeeded"\'`\n    fi\n\n    if [ "$installReady" != true ]\n    then\n        sleep 5\n    fi\ndone\n\noc create -n monitoring-grafana -f - <<EOF\napiVersion: integreatly.org/v1alpha1\nkind: Grafana\nmetadata:\n  name: grafana\nspec:\n  baseImage: docker.io/grafana/grafana-oss:9.4.7\n  config:\n    log:\n      mode: "console"\n      level: "warn"\n    auth:\n      disable_login_form: false\n      disable_signout_menu: true\n    auth.basic:\n      enabled: true\n    auth.anonymous:\n      enabled: true\n      org_role: Admin\n  deployment:\n    env:\n      - name: GRAFANA_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: grafana-auth-secret\n            key: token\n  containers:\n    - env:\n        - name: SAR\n          value: \'-openshift-sar={"resource": "namespaces", "verb": "get"}\'\n      args:\n        - \'-provider=openshift\'\n        - \'-pass-basic-auth=false\'\n        - \'-https-address=:9091\'\n        - \'-http-address=\'\n        - \'-email-domain=*\'\n        - \'-upstream=http://localhost:3000\'\n        - "\\$(SAR)"\n        - \'-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}\'\n        - \'-tls-cert=/etc/tls/private/tls.crt\'\n        - \'-tls-key=/etc/tls/private/tls.key\'\n        - \'-client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token\'\n        - \'-cookie-secret-file=/etc/proxy/secrets/session_secret\'\n        - \'-openshift-service-account=grafana-serviceaccount\'\n        - \'-openshift-ca=/etc/pki/tls/cert.pem\'\n        - \'-openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\'\n        - \'-skip-auth-regex=^/metrics\'\n      image: \'registry.redhat.io/openshift4/ose-oauth-proxy:v4.10\'\n      imagePullPolicy: Always\n      name: grafana-proxy\n      ports:\n        - containerPort: 9091\n          name: grafana-proxy\n      resources: {}\n      volumeMounts:\n        - mountPath: /etc/tls/private\n          name: secret-grafana-k8s-tls\n          readOnly: false\n        - mountPath: /etc/proxy/secrets\n          name: secret-grafana-k8s-proxy\n          readOnly: false\n  secrets:\n    - grafana-k8s-tls\n    - grafana-k8s-proxy\n  service:\n    ports:\n      - name: grafana-proxy\n        port: 9091\n        protocol: TCP\n        targetPort: grafana-proxy\n    annotations:\n      service.alpha.openshift.io/serving-cert-secret-name: grafana-k8s-tls\n  ingress:\n    enabled: true\n    targetPort: grafana-proxy\n    termination: reencrypt\n  client:\n    preferService: true\n  serviceAccount:\n    annotations:\n      serviceaccounts.openshift.io/oauth-redirectreference.primary: \'{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"grafana-route"}}\'\n  dashboardLabelSelector:\n    - matchExpressions:\n        - { key: "app", operator: In, values: [\'grafana\'] }\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: grafana-proxy\nrules:\n  - apiGroups:\n      - authentication.k8s.io\n    resources:\n      - tokenreviews\n    verbs:\n      - create\n  - apiGroups:\n      - authorization.k8s.io\n    resources:\n      - subjectaccessreviews\n    verbs:\n      - create\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: grafana-proxy\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana-proxy\nsubjects:\n  - kind: ServiceAccount\n    name: grafana-serviceaccount\n---\napiVersion: v1\ndata:\n  session_secret: Y2hhbmdlIG1lCg==\nkind: Secret\nmetadata:\n  name: grafana-k8s-proxy\ntype: Opaque\nEOF\n\nwaitForStatus \\\n  grafanas.integreatly.org \\\n  grafana \\\n  400 \\\n  10 \\\n  \'{.status.phase}\' \\\n  \'failing\'\n\n# Ignore error if svc grafana-alert has no annotations\noc patch svc grafana-alert --type=json -p=\'[{"op": "remove", "path": "/metadata/annotations"}]\' || true\noc delete secret grafana-k8s-tls\noc annotate service grafana-service fixed=yes\n\noc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount -n monitoring-grafana\n\noc create -n monitoring-grafana -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: grafana-auth-secret\n  annotations:\n    kubernetes.io/service-account.name: grafana-serviceaccount\ntype: kubernetes.io/service-account-token\nEOF\n\noc create -n monitoring-grafana -f - <<EOF\napiVersion: integreatly.org/v1alpha1\nkind: GrafanaDataSource\nmetadata:\n  name: prometheus\nspec:\n  datasources:\n    - access: proxy\n      editable: true\n      isDefault: true\n      jsonData:\n        httpHeaderName1: \'Authorization\'\n        timeInterval: 5s\n        tlsSkipVerify: true\n      name: prometheus\n      secureJsonData:\n        httpHeaderValue1: \'Bearer \\${GRAFANA_TOKEN}\'\n      type: prometheus\n      url: \'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091\'\n  name: prometheus.yaml\nEOF\n\necho "Monitoring will be available at https://$(oc get route -n monitoring-grafana grafana-route -o jsonpath=\'{.status.ingress[0].host}\')"\n')),(0,r.yg)("p",null,"Alternatively, there is a ",(0,r.yg)("a",{parentName:"p",href:"https://cloud.redhat.com/experts/o11y/ocp-grafana/"},"RedHat blog")," which details\nhow to deploy grafana and connect to Thanos."),(0,r.yg)("h4",{id:"creating-dashboards"},"Creating dashboards"),(0,r.yg)("p",null,"All the same queries listed above for the OCP console will work in Grafana too, and can be attached to any\nof the available visualizations."),(0,r.yg)("p",null,"There are some example dashboards available in\n",(0,r.yg)("a",{parentName:"p",href:"https://github.com/IBM/waiops-tech-jam/tree/main/labs/practitioner-basics/4-monitoring/example-dashboards"},"example-dashboards"),",\nwhich can be imported into Grafana."))}g.isMDXComponent=!0}}]);