"use strict";(self.webpackChunkswat_hub=self.webpackChunkswat_hub||[]).push([[2673],{15680:(e,n,t)=>{t.d(n,{xA:()=>u,yg:()=>m});var a=t(96540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),c=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},u=function(e){var n=c(e.components);return a.createElement(s.Provider,{value:n},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=c(t),g=i,m=d["".concat(s,".").concat(g)]||d[g]||p[g]||r;return t?a.createElement(m,o(o({ref:n},u),{},{components:t})):a.createElement(m,o({ref:n},u))}));function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[d]="string"==typeof e?e:i,o[1]=l;for(var c=2;c<r;c++)o[c]=t[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"},7780:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var a=t(58168),i=(t(96540),t(15680));const r={title:"Reclaiming Free Space",description:"Guidance for maintenaning your storage cluster over time.",sidebar_position:2},o="Reclaiming free space on block devices",l={unversionedId:"practitioner-basics/storage/data-foundation/maintenance",id:"practitioner-basics/storage/data-foundation/maintenance",title:"Reclaiming Free Space",description:"Guidance for maintenaning your storage cluster over time.",source:"@site/labs/practitioner-basics/3-storage/data-foundation/maintenance.mdx",sourceDirName:"practitioner-basics/3-storage/data-foundation",slug:"/practitioner-basics/storage/data-foundation/maintenance",permalink:"/waiops-tech-jam/labs/practitioner-basics/storage/data-foundation/maintenance",draft:!1,editUrl:"https://github.com/IBM/waiops-tech-jam/tree/main/labs/practitioner-basics/3-storage/data-foundation/maintenance.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Reclaiming Free Space",description:"Guidance for maintenaning your storage cluster over time.",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Planning",permalink:"/waiops-tech-jam/labs/practitioner-basics/storage/data-foundation/planning"},next:{title:"Configuring self-monitoring",permalink:"/waiops-tech-jam/labs/practitioner-basics/monitoring/configuring-self-monitoring"}},s={},c=[{value:"TL;DR",id:"tldr",level:2},{value:"Understanding block device (RBD) capacity behaviour",id:"understanding-block-device-rbd-capacity-behaviour",level:2},{value:"What happens if unused space is not reclaimed periodically?",id:"what-happens-if-unused-space-is-not-reclaimed-periodically",level:2},{value:"Why don&#39;t I need to manually run <code>fstrim</code> in a normal Linux server environment?",id:"why-dont-i-need-to-manually-run-fstrim-in-a-normal-linux-server-environment",level:2},{value:"What else can impact capacity utilization?",id:"what-else-can-impact-capacity-utilization",level:2},{value:"How frequently should <code>fstrim</code> be run?",id:"how-frequently-should-fstrim-be-run",level:2},{value:"Keeping an eye on application storage configuration changes",id:"keeping-an-eye-on-application-storage-configuration-changes",level:2},{value:"How do I configure <code>fstrim</code>?",id:"how-do-i-configure-fstrim",level:2}],u={toc:c},d="wrapper";function p(e){let{components:n,...r}=e;return(0,i.yg)(d,(0,a.A)({},u,r,{components:n,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"reclaiming-free-space-on-block-devices"},"Reclaiming free space on block devices"),(0,i.yg)("p",null,"This section focuses on the importance of reclaiming free blocks after delete\noperations periodically to avoid running out of capacity unnecessarily."),(0,i.yg)("h2",{id:"tldr"},"TL;DR"),(0,i.yg)("p",null,"Following deletion operations, freed blocks are not immediately reclaimed and\nreturned to the underlying block storage pool. Freed blocks need to be reclaimed\nas part of a normal and healthy operations regime to maintain accurate and\nefficient capacity utilization."),(0,i.yg)("p",null,"Understanding the need for the capacity reclamation process and configuring\nappropriate schedules to reclaim freed blocks will help ensure applications are\nnot suddenly impacted due to loss of storage capacity."),(0,i.yg)("p",null,"See ",(0,i.yg)("a",{parentName:"p",href:"https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.14/html/managing_and_allocating_storage_resources/reclaiming-space-on-target-volumes_rhodf#reclaiming-space-on-target-volumes_rhodf"},"Reclaiming space on target volumes")," for guidance on how to\nconfigure ",(0,i.yg)("inlineCode",{parentName:"p"},"fstrim")," jobs to help maintain healthy capacity utilization of your\nstorage cluster."),(0,i.yg)("h2",{id:"understanding-block-device-rbd-capacity-behaviour"},"Understanding block device (RBD) capacity behaviour"),(0,i.yg)("p",null,"In production environments, the recommended storage devices are enterprise grade\nSSD or NVMe drives for best performance and resiliency."),(0,i.yg)("p",null,"When configuring underlying storage in a software-defined-storage (SDS)\nsolution, it can be common practice to leverage thin-provisioning for efficiency\nand utilization."),(0,i.yg)("p",null,"When thin-provisioned, disk space is allocated based on data needs instead of\npreallocation upfront. As a result, the block storage controller has more work\nto do in maintaining the underlying blocks, e.g. managing which blocks have been\nfreed and reclaiming them for reuse in other storage requests."),(0,i.yg)("p",null,"To understand this, let's consider a simple scenario of what happens when an\napplication requests block storage using a Persistent Volume Claim (PVC) with\ndynamic provisioning via a ",(0,i.yg)("inlineCode",{parentName:"p"},"StorageClass")," in filesystem mode:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"On receiving a PVC request in a thin-provisioned storage solution, a\nPersistent Volume (PV) is created in the underlying block storage pool,\nformatted with the requested filesystem, and mounted onto the node(s) where\nthe consuming Pods are scheduled to run.")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"The application running in the associated Pod(s) read, write and delete data\nas needed.")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"When data is deleted within the filesystem, whilst it may appear free for\nreuse within the context of the PV's filesystem, the underlying block storage\npool is not aware the associated blocks are actually free yet.")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"As new writes occur, it can be the case that new blocks from the underlying\nstorage pool are allocated. This can happen for many reasons and is typically\ntransparent to the filesystem within the PV."))),(0,i.yg)("p",null,"When files are deleted from a filesystem within a PV, the occupied space is not\nreleased and returned immediately to the underlying block storage pool. Which\ncan lead to a discrepancy between the actual used space reported by the\nfilesystem inside the PV, and the space reported by the underlying storage\nplatform, in this case Ceph."),(0,i.yg)("p",null,"The process used to reclaim unused space in the underlying block storage pool\ndevices is known as ",(0,i.yg)("inlineCode",{parentName:"p"},"fstrim"),":"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"It essentially informs the underlying storage pool about blocks no longer in\nuse, allowing Ceph to reclaim the space.")),(0,i.yg)("h2",{id:"what-happens-if-unused-space-is-not-reclaimed-periodically"},"What happens if unused space is not reclaimed periodically?"),(0,i.yg)("p",null,"The immediate affect is the storage solution cannot report accurately on\nutilized disk space, which can lead to:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"A capacity threshold warning alerts being triggered at ",(0,i.yg)("inlineCode",{parentName:"li"},"75%")," full capacity."),(0,i.yg)("li",{parentName:"ul"},"A capacity threshold critical alert being triggered when capacity usage\ncrosses ",(0,i.yg)("inlineCode",{parentName:"li"},"85%"),":",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"This leads to the storage cluster going into ",(0,i.yg)("inlineCode",{parentName:"li"},"read-only")," mode for data\nprotection until the situation is resolved.")))),(0,i.yg)("p",null,"So whilst the application and filesystem inside the PV may ",(0,i.yg)("em",{parentName:"p"},"think")," they have\navailable capacity from the filesystem perspective, the underlying storage\nsolution thinks it is running out of capacity, until blocks marked to be freed\nare actually reclaimed."),(0,i.yg)("p",null,"If the storage cluster goes into ",(0,i.yg)("inlineCode",{parentName:"p"},"read-only")," mode, or backing devices begin to\nexperience instability related problems, the application relying on the storage\nwill be negatively impacted, potentially going down completely."),(0,i.yg)("admonition",{title:"Increased writes and deletes can accelerate overall capacity usage.",type:"caution"},(0,i.yg)("p",{parentName:"admonition"},"In periods of increased write activity, underlying block consumption can be\naccelerated if new writes outstrip deletes and block reclamation.")),(0,i.yg)("h2",{id:"why-dont-i-need-to-manually-run-fstrim-in-a-normal-linux-server-environment"},"Why don't I need to manually run ",(0,i.yg)("inlineCode",{parentName:"h2"},"fstrim")," in a normal Linux server environment?"),(0,i.yg)("p",null,"In a normal Linux (normal being a standard, non-Kubernetes server environment)\nsystem which uses ",(0,i.yg)("inlineCode",{parentName:"p"},"systemd"),", ",(0,i.yg)("inlineCode",{parentName:"p"},"fstrim")," is normally configured to run weekly as a\n",(0,i.yg)("inlineCode",{parentName:"p"},"timer")," job."),(0,i.yg)("p",null,"On a ",(0,i.yg)("inlineCode",{parentName:"p"},"systemd")," based Linux server that has an SSD or NVMe drive, you can list\nthe active timers using:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"sudo systemctl list-timers\n")),(0,i.yg)("p",null,"You should see output like below:"),(0,i.yg)("p",null,(0,i.yg)("img",{alt:"Systemd fstrim timer",src:t(18715).A,width:"1954",height:"360"})),(0,i.yg)("admonition",{title:"Configuration can vary between Linux distributions.",type:"note"}),(0,i.yg)("h2",{id:"what-else-can-impact-capacity-utilization"},"What else can impact capacity utilization?"),(0,i.yg)("p",null,"As mentioned above, write and delete heavy usage patterns at the filesystem\nlevel can impact how quickly capacity usage grows, especially during periods of\nhigh writes, like a data storm for example."),(0,i.yg)("p",null,"But application data isn't the only thing that consumes capacity from the\nunderlying storage pool. The storage solution itself has to maintain metadata it\nuses to manage the storage cluster."),(0,i.yg)("p",null,"As data volume increases, so does the metadata. In situations where there are\nfrequent writes or deletes, metadata can accumulate over time, particularly if\nthe storage solution is not configured to reclaim free blocks after deletions at\nan optimum frequency."),(0,i.yg)("p",null,"This all contributes to overall capacity usage of the usable storage pool."),(0,i.yg)("h2",{id:"how-frequently-should-fstrim-be-run"},"How frequently should ",(0,i.yg)("inlineCode",{parentName:"h2"},"fstrim")," be run?"),(0,i.yg)("p",null,"This depends on several factors, including application workload filesytem usage\npatterns. In most cases every ",(0,i.yg)("inlineCode",{parentName:"p"},"24 hours")," may be sufficient."),(0,i.yg)("p",null,"However, in cases where there are substantial writes and deletions at fairly\nrapid or frequent rates, more aggressive ",(0,i.yg)("inlineCode",{parentName:"p"},"fstrim")," frequency may be required."),(0,i.yg)("admonition",{title:"Consider performance impacts of running too frequently.",type:"caution"},(0,i.yg)("p",{parentName:"admonition"},"It is always advisable to seek advice from storage subject matter experts and\ntest the impact of frequency changes in a pre-production environment before\nmaking changes in production.")),(0,i.yg)("p",null,"The current recommended approach is to monitor for ",(0,i.yg)("inlineCode",{parentName:"p"},"CephClusterNearFull")," alerts\nwhich by default occur at ",(0,i.yg)("inlineCode",{parentName:"p"},"75%")," utilization:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Take action to add capacity or use it as guidance to work out the frequency of\nrunning ",(0,i.yg)("inlineCode",{parentName:"li"},"fstrim")," to reclaim space.")),(0,i.yg)("p",null,"An additional proactive approach would be to leverage Prometheus metrics to\nunderstand how fast PVs may be filling up:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"It could be the case that there is a data storm causing the capacity\nutilization to go up faster than normal:",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Leveraging rate calculations as part of the overall monitoring strategy will\nhelp provide better forward guidance.")))),(0,i.yg)("h2",{id:"keeping-an-eye-on-application-storage-configuration-changes"},"Keeping an eye on application storage configuration changes"),(0,i.yg)("p",null,"From time to time, based on application and usage requirements, there can be a\nneed to increase Persistent Volume Claim (PVC) capacity requests."),(0,i.yg)("p",null,"It is imporant to remember that in a highly abstracted system such as Kubernetes\n/ OpenShift, coupled with Software-Defined-Solutions (SDS) such as IBM Storage\nFusion Data Foundation or OpenShift Data Foundation, the storage solution will\nnot be inherently aware of increases in overall application capacity\nrequirements. The storage solution will keep attempting to service storage\nrequests until it runs out of capacity."),(0,i.yg)("p",null,"It is highly advisable to introduce a careful change management process which\nincludes reviewing any changes to PVC requirements and whether more storage\ncapacity is required as a result."),(0,i.yg)("h2",{id:"how-do-i-configure-fstrim"},"How do I configure ",(0,i.yg)("inlineCode",{parentName:"h2"},"fstrim"),"?"),(0,i.yg)("p",null,"This typically requires annotating the target PVCs with a schedule frequency."),(0,i.yg)("p",null,"It is highly advisable to review the documentation fully and testing in a\npre-production environment before making changes in production."),(0,i.yg)("p",null,"Refer to the ",(0,i.yg)("a",{parentName:"p",href:"https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.14/html/managing_and_allocating_storage_resources/reclaiming-space-on-target-volumes_rhodf#reclaiming-space-on-target-volumes_rhodf"},"Red Hat OpenShift Data Foundation")," documentation\nfor detailed instructions on the procedure."),(0,i.yg)("hr",null))}p.isMDXComponent=!0},18715:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/systemd-fstrim-timer-a323d228605866875308c6128ba1357a.png"}}]);