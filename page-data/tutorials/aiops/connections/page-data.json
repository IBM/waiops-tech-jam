{"componentChunkName":"component---src-pages-tutorials-aiops-connections-index-mdx","path":"/tutorials/aiops/connections/","result":{"pageContext":{"frontmatter":{"title":"Connect an on-premise Probe","description":"Configure and connect an on-premise Netcool Probe"},"relativePagePath":"/tutorials/aiops/connections/index.mdx","titleType":"page","MdxNode":{"id":"9574edeb-cb87-5084-85bc-2a104568c9c6","children":[],"parent":"ec88febf-d61b-5e1b-8fbf-1ce934a825fd","internal":{"content":"---\ntitle: Connect an on-premise Probe\ndescription: Configure and connect an on-premise Netcool Probe\n---\n\n<AnchorLinks>\n  <AnchorLink>3-1: Overview</AnchorLink>\n  <AnchorLink>3-2: Enable Nodeports</AnchorLink>\n  <AnchorLink>3-3: Gather the cluster connection information</AnchorLink>\n  <AnchorLink>3-4: Configure probe to connect to the cluster</AnchorLink>\n  <AnchorLink>3-5: Start Simnet probe</AnchorLink>\n</AnchorLinks>\n\n\n\n## 3-1: Overview\n\nThis module focuses on configuring and connecting an on-premise Netcool/Probe to the Event Manager.  In most demonstration or POC scenarios, there will be an existing on-premise Netcool deployment present, and there will be a need to connect that environment to the system running on OpenShift. Typically this will involve connecting Probes or uni-directional ObjectServer Gateways into the Event Manager, to provide a source of events.\n\n**NOTE:** This module uses on-premise OMNIbus probe that you want to connect into your Event Manager system. \nOMNIbus Probe server is pre-installed for you on VM (pi-template) in your environment, including Simnet Probe under /home/scadmin/IBM/tivoli/netcool\nYou can connect to this VM using `ssh pi-template` from your control node.\n\nThis deployment scenario will assume you are deploying onto IBM Cloud however the steps would generally apply to an OpenShift cluster deployed on another cloud provider or on-premise.\n\nBy the end of this module, you will have enabled the ObjectServer nodeports on your cluster, configured your on-premise Netcool/OMNIbus system to connect to the ObjectServer embedded within Event Manager, and connected a Probe.\n\n\n## 3-2: Enable Nodeports\n\nBehind the scenes, the event stores in Watson for AIOps Event Manager are Netcool/OMNIbus ObjectServers. These are running in containers and are not accessible outside the cluster by default. This first step involves modifying your Event Manager deployment to activate the nodeports.\n\nLog in to your OpenShift UI and navigate to: **Operators → Installed Operators → IBM Cloud Pak for Watson AIOps Event Manager → NOI**.\n\nNext, click on your deployment - for example: **evtmanager** - and then click on the **YAML** tab.\n\nAdd a new sub-section to the **spec**: section in your configuration with the following text:\n\n```sh\n  helmValuesNOI:\n    global.service.nodePort.enable: true\n```\n\n**NOTE**: the spacing and indentation is important here.\n\nAfter you have added this sub-section and clicked Save, the resulting configuration should look something like the following:\n\n![](images/0_yaml.png)\n\n**NOTE:** In this screenshot, other properties have also been added. The key added lines are only lines **196** and **198**.\n\nAfter you have saved the configuration, OpenShift will detect the change, and redeploy the relevant services.\n\nFor more information on this step, see the following documentation link: https://www.ibm.com/docs/en/noi/1.6.5?topic=service-identifying-proxy-listening-port\n\n## 3-3: Gather the cluster connection information\n\nThis step involves compiling information about the cluster's certificate common name (CN) and acquiring the cluster certificate for import into the Probe server (pi-template).\n\n**a) Find IP address of Cluster Ingress documentation**\n\nIn the previous module 2-6 , you identified the Ingress subdomain from the IBM Cloud UI, from the information relating to your cluster. From the control node execute following commands:\n\nPing the Ingress subdomain to find the **IP address** to use to communicate with the cluster:\n\n```sh\n\nping swat01-4693fb98e216XXXXXXXXXXX-0000.us-east.containers.appdomain.cloud\n\nPING swat01-4693fb98e21XXXXXXXXXXXXx-0000.us-east.containers.appdomain.cloud (5X.XXX.XX.XXX) 56(84) bytes of data.\n64 bytes from evtmanager-proxy.noi.svc (5X.XXX.XX.XXX): icmp_seq=1 ttl=46 time=89.2 ms\n^C\n--- swat01-4693fb98e216d6XXXXXXXXXXXXXX-0000.us-east.containers.appdomain.cloud ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1001ms\nrtt min/avg/max/mdev = 89.216/93.100/96.985/3.896 ms\n```\n\nWe will use IP address: 5X.XXX.XX.XXX from above output to communicate with the cluster.\n\n\n**b) Find Nodeport for proxy server running in OCP cluster**\n\nNext, log in to the OpenShift cluster via the command line, as you did in the previous module. To check that the nodeports have been successfully deployed, you can query the Proxy Service, and check its output for the nodeport values:\n\n```sh\noc get service -o yaml evtmanager-proxy -n noi | grep nodePort\n```\nOutput looks like :\n\n`\n    nodePort: 30846\n    nodePort: 31746\n`\n\n\nFrom the output above, you can see that the nodeports have been deployed on ports **30846** for the primary ObjectServer and **31846** for the backup. These are the port numbers that are externally accessible to the cluster and are what your Netcool Probe (or in-bound Netcool Gateway) will use to connect.\n\nc) **Extract Openshift cluster certificate**\n\nUse the OpenSSL command to retrieve the x.509 certificate that is returned by the proxy and verify the certificate common name (CN). You need to again use the Ingress subdomain value in conjunction with the nodeport value you discovered in Step 1:\n\n```sh\nopenssl s_client -showcerts -connect swat01-469XXXXXXXXXXXXXXXXXXXXXXXXXc049-0000.us-east.containers.appdomain.cloud:32767\n```\n\n`CONNECTED(00000003)\ndepth=1 CN = openshift-service-serving-signer@1645115959\nverify return:1\ndepth=1 CN = openshift-service-serving-signer@1645115959\nverify return:1\ndepth=0 CN = evtmanager-proxy.noi.svc\nverify return:1\nCertificate chain\n0 s:CN = evtmanager-proxy.noi.svc\n...\n`\n\nHere you can see the proxy service name **evtmanager-proxy.noi.svc** being used by the server, that is associated with the certificate, and that it matches the hostname returned by the ping command earlier. This is important because the hostname we use to connect to the cluster must match that referred to in the certificate in order for the SSL connection to work correctly.\n\nFinally, download the certificate from the cluster using the oc utility:\n\n```sh\noc get secrets/signing-key -n openshift-service-ca -o template='{{index .data \"tls.crt\"}}' | base64 --decode > cluster-ca-cert.pem \n```\n\nCopy this file (cluster-ca-cert.pem) to the Probe server (pi-template) in preparation for the next step.\n\n```sh\nscp cluster-ca-cert.pem jammer@pi-template:/tmp/\n```\n\nFor more information on this step, see the following documentation link: https://www.ibm.com/docs/en/noi/1.6.5?topic=service-configuring-tls-encryption-red-hat-openshift\n\n## 3-4: Configure probe to connect to the cluster\n\nNow that we have the proxy service hostname, the nodeport of the primary ObjectServer, and the cluster's certificate, we are ready to configure the On-premise Probe to connect to the Event Manager running in OCP.\n\n\na) Log in to the OMNIBus Probe server (pi-template)\n\n```sh\nssh pi-template\n```\n\nb) Add an entry to the /etc/hosts file with the cluster IP address and the proxy service hostname obtained previously:\n\nFrom control node ssh into pi-template VM\n\n```sh\nsudo vi /etc/hosts\n\n```\n\n5X.XXX.XX.XXX evtmanager-proxy.noi.svc\n\nThis is the hostname (evtmanager-proxy.noi.svc) that we are going to use in omni.dat interfaces file.\n\nc) Add new interfaces entry in $NCHOME/etc/omni.dat, representing the primary ObjectServer running in OpenShift (please do not delete exising entries):\n\n```sh\n[ROKS_AGG_P]\n{\n Primary: evtmanager-proxy.noi.svc ssl 32767\n}\n```\n\n**NOTE:** The entry contains the string ssl which indicates that an encrypted connection should be used.\n\nd) Run nco_igen to update the interfaces file information.\n\n```sh\n$NCHOME/bin/nco_igen\n```\n\ne) Create Keystore for Cerificate:\n\nIf you haven't already created one previously, create a keystore on your Probe server to import the certificate into:\n\n```sh\nexport NCHOME=/home/scadmin/IBM/tivoli/netcool\n$NCHOME/bin/nc_gskcmd -keydb -create -db \"$NCHOME/etc/security/keys/omni.kdb\" -pw password -stash -expire 1000\n```\n\nf) Copy the certificate you downloaded in Step 2 to the Probe server, and import it into your newly created keystore:\n\n```sh\n$NCHOME/bin/nc_gskcmd -cert -add -file /tmp/cluster-ca-cert.pem -db $NCHOME/etc/security/keys/omni.kdb -stashed\n```\n\n\ng) Use the nco_ping utility to test the connection the ObjectServer:\n\n```sh\n$NCHOME/omnibus/bin/nco_ping ROKS_AGG_P\n```\nOutput:\n`NCO_PING: Server available.`\n\nYou are now ready to connect your Probe.\n\nFor more information on this step, see the following documentation link: https://www.ibm.com/docs/en/noi/1.6.5?topic=service-configuring-tls-encryption-red-hat-openshift\n\n## 3-5: Start Simnet probe\n\nThe final step is to configure the Probe to connect to the primary ObjectServer running in OpenShift. \n\nUsing the Simnet Probe as an example, run the Probe in debug mode to ensure that the Probe can connect to the ObjectServer:\n\n```sh\nexport OMNIHOME=/home/scadmin/IBM/tivoli/netcool/omnibus\n$OMNIHOME/probes/nco_p_simnet -server ROKS_AGG_P -messagelevel debug -messagelog stdout\n```\n\nYou can check events in Event viewer sent via Simnet probe.\n\nAfter verifying successful connect to the ObjectServer, you can run the Probe outside of debug mode, normally under Process Agent control.\n\n\n[Reference Blog](https://community.ibm.com/community/user/aiops/blogs/zane-bray1/2022/04/13/getting-started-with-watson-aiops-event-manager-27)","type":"Mdx","contentDigest":"96e27550fde718f9aed57d648e0bba41","owner":"gatsby-plugin-mdx","counter":927},"frontmatter":{"title":"Connect an on-premise Probe","description":"Configure and connect an on-premise Netcool Probe"},"exports":{},"rawBody":"---\ntitle: Connect an on-premise Probe\ndescription: Configure and connect an on-premise Netcool Probe\n---\n\n<AnchorLinks>\n  <AnchorLink>3-1: Overview</AnchorLink>\n  <AnchorLink>3-2: Enable Nodeports</AnchorLink>\n  <AnchorLink>3-3: Gather the cluster connection information</AnchorLink>\n  <AnchorLink>3-4: Configure probe to connect to the cluster</AnchorLink>\n  <AnchorLink>3-5: Start Simnet probe</AnchorLink>\n</AnchorLinks>\n\n\n\n## 3-1: Overview\n\nThis module focuses on configuring and connecting an on-premise Netcool/Probe to the Event Manager.  In most demonstration or POC scenarios, there will be an existing on-premise Netcool deployment present, and there will be a need to connect that environment to the system running on OpenShift. Typically this will involve connecting Probes or uni-directional ObjectServer Gateways into the Event Manager, to provide a source of events.\n\n**NOTE:** This module uses on-premise OMNIbus probe that you want to connect into your Event Manager system. \nOMNIbus Probe server is pre-installed for you on VM (pi-template) in your environment, including Simnet Probe under /home/scadmin/IBM/tivoli/netcool\nYou can connect to this VM using `ssh pi-template` from your control node.\n\nThis deployment scenario will assume you are deploying onto IBM Cloud however the steps would generally apply to an OpenShift cluster deployed on another cloud provider or on-premise.\n\nBy the end of this module, you will have enabled the ObjectServer nodeports on your cluster, configured your on-premise Netcool/OMNIbus system to connect to the ObjectServer embedded within Event Manager, and connected a Probe.\n\n\n## 3-2: Enable Nodeports\n\nBehind the scenes, the event stores in Watson for AIOps Event Manager are Netcool/OMNIbus ObjectServers. These are running in containers and are not accessible outside the cluster by default. This first step involves modifying your Event Manager deployment to activate the nodeports.\n\nLog in to your OpenShift UI and navigate to: **Operators → Installed Operators → IBM Cloud Pak for Watson AIOps Event Manager → NOI**.\n\nNext, click on your deployment - for example: **evtmanager** - and then click on the **YAML** tab.\n\nAdd a new sub-section to the **spec**: section in your configuration with the following text:\n\n```sh\n  helmValuesNOI:\n    global.service.nodePort.enable: true\n```\n\n**NOTE**: the spacing and indentation is important here.\n\nAfter you have added this sub-section and clicked Save, the resulting configuration should look something like the following:\n\n![](images/0_yaml.png)\n\n**NOTE:** In this screenshot, other properties have also been added. The key added lines are only lines **196** and **198**.\n\nAfter you have saved the configuration, OpenShift will detect the change, and redeploy the relevant services.\n\nFor more information on this step, see the following documentation link: https://www.ibm.com/docs/en/noi/1.6.5?topic=service-identifying-proxy-listening-port\n\n## 3-3: Gather the cluster connection information\n\nThis step involves compiling information about the cluster's certificate common name (CN) and acquiring the cluster certificate for import into the Probe server (pi-template).\n\n**a) Find IP address of Cluster Ingress documentation**\n\nIn the previous module 2-6 , you identified the Ingress subdomain from the IBM Cloud UI, from the information relating to your cluster. From the control node execute following commands:\n\nPing the Ingress subdomain to find the **IP address** to use to communicate with the cluster:\n\n```sh\n\nping swat01-4693fb98e216XXXXXXXXXXX-0000.us-east.containers.appdomain.cloud\n\nPING swat01-4693fb98e21XXXXXXXXXXXXx-0000.us-east.containers.appdomain.cloud (5X.XXX.XX.XXX) 56(84) bytes of data.\n64 bytes from evtmanager-proxy.noi.svc (5X.XXX.XX.XXX): icmp_seq=1 ttl=46 time=89.2 ms\n^C\n--- swat01-4693fb98e216d6XXXXXXXXXXXXXX-0000.us-east.containers.appdomain.cloud ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1001ms\nrtt min/avg/max/mdev = 89.216/93.100/96.985/3.896 ms\n```\n\nWe will use IP address: 5X.XXX.XX.XXX from above output to communicate with the cluster.\n\n\n**b) Find Nodeport for proxy server running in OCP cluster**\n\nNext, log in to the OpenShift cluster via the command line, as you did in the previous module. To check that the nodeports have been successfully deployed, you can query the Proxy Service, and check its output for the nodeport values:\n\n```sh\noc get service -o yaml evtmanager-proxy -n noi | grep nodePort\n```\nOutput looks like :\n\n`\n    nodePort: 30846\n    nodePort: 31746\n`\n\n\nFrom the output above, you can see that the nodeports have been deployed on ports **30846** for the primary ObjectServer and **31846** for the backup. These are the port numbers that are externally accessible to the cluster and are what your Netcool Probe (or in-bound Netcool Gateway) will use to connect.\n\nc) **Extract Openshift cluster certificate**\n\nUse the OpenSSL command to retrieve the x.509 certificate that is returned by the proxy and verify the certificate common name (CN). You need to again use the Ingress subdomain value in conjunction with the nodeport value you discovered in Step 1:\n\n```sh\nopenssl s_client -showcerts -connect swat01-469XXXXXXXXXXXXXXXXXXXXXXXXXc049-0000.us-east.containers.appdomain.cloud:32767\n```\n\n`CONNECTED(00000003)\ndepth=1 CN = openshift-service-serving-signer@1645115959\nverify return:1\ndepth=1 CN = openshift-service-serving-signer@1645115959\nverify return:1\ndepth=0 CN = evtmanager-proxy.noi.svc\nverify return:1\nCertificate chain\n0 s:CN = evtmanager-proxy.noi.svc\n...\n`\n\nHere you can see the proxy service name **evtmanager-proxy.noi.svc** being used by the server, that is associated with the certificate, and that it matches the hostname returned by the ping command earlier. This is important because the hostname we use to connect to the cluster must match that referred to in the certificate in order for the SSL connection to work correctly.\n\nFinally, download the certificate from the cluster using the oc utility:\n\n```sh\noc get secrets/signing-key -n openshift-service-ca -o template='{{index .data \"tls.crt\"}}' | base64 --decode > cluster-ca-cert.pem \n```\n\nCopy this file (cluster-ca-cert.pem) to the Probe server (pi-template) in preparation for the next step.\n\n```sh\nscp cluster-ca-cert.pem jammer@pi-template:/tmp/\n```\n\nFor more information on this step, see the following documentation link: https://www.ibm.com/docs/en/noi/1.6.5?topic=service-configuring-tls-encryption-red-hat-openshift\n\n## 3-4: Configure probe to connect to the cluster\n\nNow that we have the proxy service hostname, the nodeport of the primary ObjectServer, and the cluster's certificate, we are ready to configure the On-premise Probe to connect to the Event Manager running in OCP.\n\n\na) Log in to the OMNIBus Probe server (pi-template)\n\n```sh\nssh pi-template\n```\n\nb) Add an entry to the /etc/hosts file with the cluster IP address and the proxy service hostname obtained previously:\n\nFrom control node ssh into pi-template VM\n\n```sh\nsudo vi /etc/hosts\n\n```\n\n5X.XXX.XX.XXX evtmanager-proxy.noi.svc\n\nThis is the hostname (evtmanager-proxy.noi.svc) that we are going to use in omni.dat interfaces file.\n\nc) Add new interfaces entry in $NCHOME/etc/omni.dat, representing the primary ObjectServer running in OpenShift (please do not delete exising entries):\n\n```sh\n[ROKS_AGG_P]\n{\n Primary: evtmanager-proxy.noi.svc ssl 32767\n}\n```\n\n**NOTE:** The entry contains the string ssl which indicates that an encrypted connection should be used.\n\nd) Run nco_igen to update the interfaces file information.\n\n```sh\n$NCHOME/bin/nco_igen\n```\n\ne) Create Keystore for Cerificate:\n\nIf you haven't already created one previously, create a keystore on your Probe server to import the certificate into:\n\n```sh\nexport NCHOME=/home/scadmin/IBM/tivoli/netcool\n$NCHOME/bin/nc_gskcmd -keydb -create -db \"$NCHOME/etc/security/keys/omni.kdb\" -pw password -stash -expire 1000\n```\n\nf) Copy the certificate you downloaded in Step 2 to the Probe server, and import it into your newly created keystore:\n\n```sh\n$NCHOME/bin/nc_gskcmd -cert -add -file /tmp/cluster-ca-cert.pem -db $NCHOME/etc/security/keys/omni.kdb -stashed\n```\n\n\ng) Use the nco_ping utility to test the connection the ObjectServer:\n\n```sh\n$NCHOME/omnibus/bin/nco_ping ROKS_AGG_P\n```\nOutput:\n`NCO_PING: Server available.`\n\nYou are now ready to connect your Probe.\n\nFor more information on this step, see the following documentation link: https://www.ibm.com/docs/en/noi/1.6.5?topic=service-configuring-tls-encryption-red-hat-openshift\n\n## 3-5: Start Simnet probe\n\nThe final step is to configure the Probe to connect to the primary ObjectServer running in OpenShift. \n\nUsing the Simnet Probe as an example, run the Probe in debug mode to ensure that the Probe can connect to the ObjectServer:\n\n```sh\nexport OMNIHOME=/home/scadmin/IBM/tivoli/netcool/omnibus\n$OMNIHOME/probes/nco_p_simnet -server ROKS_AGG_P -messagelevel debug -messagelog stdout\n```\n\nYou can check events in Event viewer sent via Simnet probe.\n\nAfter verifying successful connect to the ObjectServer, you can run the Probe outside of debug mode, normally under Process Agent control.\n\n\n[Reference Blog](https://community.ibm.com/community/user/aiops/blogs/zane-bray1/2022/04/13/getting-started-with-watson-aiops-event-manager-27)","fileAbsolutePath":"/home/amar/workspace/git-repos/github.com/IBM/waiops-tech-jam/src/pages/tutorials/aiops/connections/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}