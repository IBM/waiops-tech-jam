{"componentChunkName":"component---src-pages-tutorials-mm-mm-mediating-csv-index-mdx","path":"/tutorials/mm/mm-mediating-csv/","result":{"pageContext":{"frontmatter":{"title":"Mediating CSV Metric Data","description":"This lab will illustrate how to analyze CSV file structure, how to create a Metric Manager topic model, how to deploy the model to a topic, and how to start ingestion of CSV data for machine learning"},"relativePagePath":"/tutorials/mm/mm-mediating-csv/index.mdx","titleType":"page","MdxNode":{"id":"c9acaced-a115-5d02-b175-c58ca6e085d4","children":[],"parent":"3f20a740-4f43-506b-a851-b695776b0529","internal":{"content":"---\ntitle: Mediating CSV Metric Data\ndescription: This lab will illustrate how to analyze CSV file structure, how to create a Metric Manager topic model, how to deploy the model to a topic, and how to start ingestion of CSV data for machine learning\n---\n\n<AnchorLinks>\n  <AnchorLink>3-1: Lab Introduction</AnchorLink>\n  <AnchorLink>3-2: Creating a topic</AnchorLink>\n  <AnchorLink>3-3: Topic configuration</AnchorLink>\n  <AnchorLink>3-4: Creating a model to ingest CSV</AnchorLink>\n  <AnchorLink>3-5: Deploying the model</AnchorLink>\n  <AnchorLink>3-6: Ingesting CSV data for machine learning</AnchorLink>\n</AnchorLinks>\n\n## 3-1: **Introduction**\n\nThe Metric Manager can consume data in four primary ways: Modelling/Mediating data that is in CSV format, Modelling/Mediating data that resides in a database via JDBC connection, and ingesting pre-formed JSON through either the REST mediation service or Kafka.\n\nThis lab will be showing you how to consume data from CSV files. There are CSV files located in the _/home/scadmin/BookInfoDemo/data/bookinfo/data_ directory. These files contain metrics for Application response time (APP), CPU utilization (CPU), Database transaction volume (DB), and disk utilization (DISKBUSY). This data spans a time period of 17 days.\n\n We will create a Metric Manager topic to receive this data, create a model for this data with the Mediation Tool, deploy that model, then start the topic and start ingesting the data.\n\n## 3-2: **Creating a topic**\n\n\n  To create a topic, open a terminal window and run the following commands:\n\n```sh\ncd /opt/IBM/scanalytics/analytics/bin\n./admin.sh create_topic BIGBLUE\n```\n\n This will take a minute or two while the Metric Manager builds the topic tables in the DB2 database and creates the appropriate analytics/streams configuration.\n\n Once the **create\\_topic** command completes, you can list the topics that are configured on the system by issuing the following command. You should see your new BIGBLUE topic, along with the TEST topic that we created to verify the iFix install:\n\n```sh\n./admin.sh show_topics\nTEST\nBIGBLUE\n```\n\n You may delete the TEST topic you created earlier to verify that the update completed successfully if you wish.\n\n```sh\n./admin.sh delete_topic TEST\n```\n\n## 3-3: **Topic configuration**\n\n### Topic configuration considerations\n\n_Aggregation interval_\n\n There are considerations for topic configuration that need to be accounted for based on the data that the Metric Manager will be ingesting. One of the more important aspects of topic configurations is the system aggregation interval. The aggregation interval defines the rate, in minutes, of the metric data that Metric Manager will be consuming. This is largely dictated by the source of the metric data (e.g. an APM tool, network performance monitor, etc). The supported PI intervals are 5, 10, 15, and 60 minutes. If the source that is generating metrics for MM ingestion is collecting at 5 minutes, it is best to set the aggregation interval for the topic to 5. If the metrics are generated at 1 minute intervals, set the topic to 5 minutes, and PI will automatically aggregate the data to the 5 minute interval.\n\n For 5 minute data, time intervals in minutes/seconds are:\n\n 00:00 – 04:59  \n 05:00 – 09:59  \n 10:00 – 14:59  \n …  \n …  \n 50:00 – 54:59  \n 55:00 – 55:50  \n\n Likewise, 10 minute intervals are:\n\n 00:00 – 09:59  \n 10:00 – 19:59  \n …  \n …  \n 40:00 - 49:49  \n 50:00 – 59:59  \n\n Investigate the data that we will model and ingest. Change your directory to where our CSV files are located and list the contents of the directory:\n\n```sh\ncd /home/scadmin/BookInfoDemo/data/bookinfo/data\nls\n```\n\n_CSV file naming_\n\nYou'll note that there are a number of csv files in that directory that look to adhere to a standard naming convention. There are rules that you must follow for naming these files, and those rules are documented in the \"IBM Operations Analytics Predictive Insights\" documentation at this link:\n\n<a href=\"https://www.ibm.com/docs/en/oapi/1.3.6?topic=SSJQQ3\\_1.3.6/com.ibm.scapi.doc/admin\\_guide/r\\_tasp\\_csvrules.html\" target=\"_blank\">Rules for using CSV data sources</a>\n\n In short:\n\n - the file name must start with a group name, and contain a delimiter\n - next, the file name must contain the start date for the data residing in the csv file, and another delimiter\n - the end date is optional if the data that resides in the CSV file is for a single interval, but if data within the CSV file spans multiple intervals, it is required\n - the file name must end in .csv\n\n The files names that we have created for execution of this lab have the format:\n\n`<GROUP>__<STARTDATE>-<STARTTIME>__<ENDDATE>-<ENDTIME>.csv`\n\n You will note that some groups have individual daily files (CPU, Diskbusy), and others have one csv file for the entire span of data (APP, DB). The Metric Manager extraction process is smart enough to read data for each of its intervals from the files simply based on the timestamp of the files that exist, as well as the timestamps within the files. Since we will be ingesting files in 'backlog' historical mode, and not live, having daily files or a file that spans 17 days is fine. If we were ingesting live data for anomaly detection, Metric Manager would expect to have a single file that contains all metric data for each aggregation interval. In live ingestion, the extractor instance will pick up the latest file for the previous aggregation interval (e.g. 00:00-04:59) and process it.\n\n So in looking at the file names, we can see that the data in this directory starts at October 1st at midnight, 2020, and ends on October 17th, noon, or about 17 days. Next, let's look at the interval of the data contained in the files.\n\n_Resources within Metric Manager_\n\nAny data sent into Metric Manager (whether it be CSV, or REST/Kafka) must contain enough information to properly classify the component that the metric is related to. For example, you may be collecting metrics for a router. That router may have multiple interfaces. There are metrics that are applicable to the router **node** itself (e.g. CPU utilization, buffer utilization), and there are metrics that are applicable to an **interface** on that router (e.g. packet drops, in bits per second, out bits per second, etc). \n\n Run the following command to page through the data contained in the APP file:\n\n```sh\nmore APP__20201001-0000__20201017-1200.csv\n```\n\nNote that the file has the required CSV header, which must be the first entry of the CSV file:\n\n**_Time,RESOURCE,ResponseTime_**\n\n In looking at the file, it's clear that the timestamp is the first column, and looking at the times they are timestamped at 5 minute intervals. Look at the other files and confirm that they are also in 5 minute intervals with the exception of DISKBUSY, which appear to be in minute intervals. That is fine as MM will aggregate our data for us (more on this later).\n\n Also note that the timestamps are in a different format than the file name. It is in the format \"Day/Month/Year Hour:Minute\", using literal slashes, space, and colon as delimiters. Additionally the Year is in 2 digit format, rather than four. So during our model creation, we must use the notation \"dd/MM/yy HH:mm\". It is important to be cognizant of timestamp formats whether they be in the file name, or within the CSV file itself. They may not always be the same, and we will take this into account when we create our model for the CSV data.\n\n### Configuring topic options\n\n Given the velocity of the data contained in these CSV files, we will need to configure the Metric Manager topic aggregation interval to 5 minutes. To set the aggregation interval, first check to see what it is currently set to:\n\n```sh\ncd /opt/IBM/scanalytics/analytics/bin\n./admin.sh show -t=BIGBLUE\n```\n\nThis will show all of the configurable options for the BIGBLUE topic. The aggregation interval is \"system.aggregation.interval\". You can see it is currently set to 15 minutes. To change it to 5 minutes, issue the following command:\n\n```sh\n./admin.sh set -t=BIGBLUE system.aggregation.interval 5\n```\n\nTo verify the change, run the following command:\n\n```sh\n./admin.sh show -t=BIGBLUE |grep system.aggregation.interval\n```\n\n... you should see that the system.aggregation.interval property is 5 minutes:\n\n`system.aggregation.interval: 5`\n\n\nWhen ingesting historical data, it's important to configure the topic such that it doesn't automatically delete the data you ingest. Metric Manager is not meant to be used as a long-term metric data repository... it is meant to keep the data long enough to successfully identify changes in behavior, and to keep a small amount of historical for short-term review. By default, a topic's metric data is cleared out after 15 days, while anomaly alarm data is kept for 180 days. Since we have data originating from 2020, we need to set a couple more topic parameters to ensure the system doesn't age out our data. Set the following options for the topic BIGBLUE:\n\n```sh\n./admin.sh set -t=BIGBLUE sta.corr.retention.days 1095\n./admin.sh set -t=BIGBLUE system.alarm.history.retention.days 1095\n./admin.sh set -t=BIGBLUE system.metric.retention.days 1095\n```\n\nAnother thing you may need to do during a proof of concept is disable alarm clearing. This is required to be disabled if you are analyzing past data and need to demonstrate what the anomaly alerts that arrive in Event Manager look like, without having them clear when the anomaly occurrence stops. Since we'll be viewing Event Manager events as part of our labs, we'll disable it:\n\n```sh\n./admin.sh set -t=BIGBLUE system.alarm.autoclear false\n```\n\n Finally, double-check and verify the topic configuration by running the show topic command:\n\n```sh\n./admin.sh show -t=BIGBLUE\n```\n\n ![](images/Picture1.png)\n\n\n Topic configuration is complete!\n\n## 3-4: **Creating a model to ingest CSV**\n\n The Mediation Tool is the component you will use to model the data contained within the CSV files so that Metric Manager can ingest, process, learn, and identify anomalies within the data. The tool allows you to create a 'model' of the structure of the CSV files you'll be working with, from within a user-friendly Eclipse interface. You can start the Mediation Tool by issuing the following commands from your VNC session terminal:\n\n```sh\ncd /opt/IBM/scanalytics/mediationtool/eclipse\n./eclipse\n```\n\n You will be presented with the initial config screen. Leave the default and click \"Ok\".\n\n ![](images/Picture2.png)\n\n\n You will be presented with the main window. In this step we will create a new Predictive Insights project. A project is simply a folder that contains one or more model definitions. From the main window select **File-\\>New-\\>Project** from the menu bar:\n\n ![](images/Picture3.png)\n\n In the next screen, open the **Predictive Insights Wizards** tree, select **Predictive Insights Project**, and click **Next**:\n\n ![](images/Picture4.png)\n\n The next screen will prompt you for a project name. Enter **TECHJAM2022** and click the **Finish** button.\n\n ![](images/Picture5.png)\n\n Next, you will be asked to switch the perspective to Predictive Insights. Click **Yes**.\n\n ![](images/Picture6.png)\n\n Next, close the welcome screen by clicking on the X in the tab above the text **Welcome to Eclipse**:\n\n ![](images/Picture7.png)\n\n Finally, you will be in your modelling view. Maximize the window to take up the full screen, you will need the real estate:\n\n ![](images/Picture8.png)\n\n### Create a data source\n\n  Next, we will create a data source. Right click on your **TECHJAM2022** project in the navigator, and select **New -\\> Predictive Insights Data Source**\n\n ![](images/Picture9.png)\n\n In the dialog window that appears, change the **Filename** parameter to **BIGBLUE.pamodel** and click **Next**.\n\n ![](images/Picture10.png)\n\n In the next screen, you will select **File System** from the drop-down box, and select **Metric names in column headers**.\n\n ![](images/Picture11.png)\n\n Click **Finish**. You now have a data source that we will configure to ingest our csv files.\n\n_A note about CSV structure: CSV files can be in either **wide** or **skinny** format. **Wide** format specifies the metric name in the CSV header. For example, in our data the CPU group has the following header:\n\n `\"TIME\",\"RESOURCE\",\"User%\",\"Wait%\"`  \n `01/10/20 00:00,\"details\",84.9,0`  \n `…`  \n `…`  \n\n Above we can see there are two metric definitions in the csv header, \"User%\" and \"Wait%\". That indicates that a single CSV line contains multiple metric definitions per resource. Hence, we selected **Metric names in column headers**.\n\n You may come across a situation where the metric name appears in the csv file data rather than the header. Compare the previous example with the following:\n\n `\"TIME\",\"RESOURCE\",\"METRICNAME\"`  \n `01/10/20 03:20,details,User%,84.9`  \n `01/10/20 03:20,details,Wait%,0`  \n `…`  \n `…`  \n\n The CSV contains a generic \"METRICNAME\" indicator, and the various metric name appears within the csv lines, rather than the header. This is **skinny** format, and each line contains only one metric. When you model the CSV file, and select the metric column, it will show all of the available unique metric names as available, and you can include/exclude metrics as needed.\n\n_Note: If you need to ingest files in both skinny and wide format, you must create separate data sources for each type, as you cannot match skinny and wide formats in a single project. You can deploy multiple CSV data sources to a single topic, but they must be done as the same deployment action – that is you must multi-select the data sources, then select \"deploy model\"._\n\n### Configure the filesystem details.\n\n The file system details define where the files are located and the structure and date format of the file names. \n\n You will be at the following screen:\n\n ![](images/Picture12.png)\n\n First, under **File System Details**, change the **File Path** to:\n\n**/home/scadmin/BookInfoDemo/data/bookinfo/data**\n\n This tells the mediation tool where to expect the CSV files.\n\n Next, modify the name pattern. As we learned previously, a CSV file name must start with a **group reference**, followed by a **delimiter**, followed next by a **start time** and  an **end time** (with the end time only strictly necessary when the file contains data for a span of multiple intervals). When defining the name pattern, we use regular expressions to match. To keep it simple, use the following regex to define the name pattern:\n\n**(.\\*)\\_\\_(.\\*)\\_\\_(.\\*)\\\\.csv**\n\n\n The mediation tool, as well as the Metric Manager extractor, uses the first regex match as the metric group identifier, the second as the Start Time, and, if defined, the third regex match as an end time.\n\n Next we will change the time format. The time format uses standard Java time format notation. Looking at our CSV file timestamps, we see that they are formatted as such:\n\n 20201001-0000\n\n Which, in Java time format notation, is:\n\n yyyyMMdd-HHmm\n\n As such, change the time format to \"yyyyMMdd-HHmm\"\n\nThe files provided are in GMT already so we can leave the timezone as it is.\n\n _Note: It is important to be cognizant of the timezone of the data and configure this appropriately, so that the analytics can properly learn the differences between weekday and weekend behavior patterns, as well as proper correlation of metrics across all CSV files. In many cases the timezone of the data you work with will be GMT, but you may run into cases where they are provided in some local time zone. Make sure that you verify with the customer what timezone the data timestamps are in._\n\n Once all the File System Details options have been configured, click the \"Test Matching\". You should get green checkmarks for the files in our directory indicating that the name pattern and time format worked properly:\n\n ![](images/Picture13.png)\n\n If you see errors indicating 'pattern mismatch', or do not see the green checkmarks, double-check your configuration and try again. If you sill have difficulty consult a lab proctor.\n\n\n### Defining the model from the data\n\n The next step is to actually build the model from the CSV file structures. Below the **File System Details** window that we just finished configuring and testing, you will see three tabs. Select the **Model Design** tab and click **Ok** in the dialog window that pops up.\n\n ![](images/Picture14.png)\n\n\n The model design window shows the configured data source (ITM) that we just finished configuring. Right-click on **ITM**, and select **Synchronize Schema**. This will cause the mediation tool to examine the structure and contents of the CSV files that match the pattern we defined in the previous section:\n\n ![](images/Picture15.png)\n\n\n The mediation tool will then churn through the available files in our defined directory, analyze the CSV structure, and present us with the metric groups it found in our directory. In the preview window, select **Add New Elements**, and it will select all of the CSV file structures it found in that directory.\n\n ![](images/Picture16.png)\n\n select all of the tables and click **Ok**.\n\n Next, for our ITM data source, open up the tree view that is now available:\n\n ![](images/Picture17.png)\n\n Expanding the tree view **fully** shows all of the available tables and columns in our CSV files.:\n\n ![](images/Picture18.png)\n\n Next, we will create metric groups for each of these. Right-click on **APP**, and select **New metric group**:\n\n ![](images/Picture19.png)\n\n\n You have the option of naming the metric group to anything that is descriptive of the type of data in the respective group. In our case, we'll call it **ApplicationPerformance**, and click **Ok**:\n\n ![](images/Picture20.png)\n\n The next screen will show an attempt by the modelling tool to create a model based on the composition of the CSV header:\n\n ![](images/Picture21.png)\n\n The mediation tool did a good job of providing a model based on the structure for this instance. It selected the proper CSV column for the **Timestamp (Time)**, the proper CSV column for the **Resource Key (RESOURCE)**, and the proper column for the **Metric Name (ResponseTime)**. So in this case, there isn't any additional work we need to do to define the model. But we do need to configure the timestamp format (as we did in the File Details definition). To do this, click select the **Model Properties** tab below the **Model Design** window. Open up the **Model -\\> ApplicationPerformance -\\> Timestamp** entry, and change the **Data Type** to **String** if it isn't already.\n\n Recall our earlier examination of the CSV file content. We discovered that the time format of the timestamp within the CSV file is different that it is in the filename. As such we must define **Time Format** as **dd/MM/yy HH:mm** (don't forget the space between the date and time). When you are finished entering the Time Format field, hit the **enter** key so that the entry is saved.\n\n ![](images/Picture22.png)\n\n Finally, click on the **Model Design** tab to return to the model design window, and click on the **preview data** icon to verify that the model is working:\n\n ![](images/Picture23.png)\n\n You should see a preview of the data extraction in the **Data Extraction Preview** tab:\n\n ![](images/Picture24.png)\n\n If you do not see data in the preview after clicking on the **Preview extraction** icon, verify the configuration of your timestamp in the model properties tab. If you feel you have verified everything and are still having issues, consult a lab proctor.\n\n Finally, take a moment to save the work we have done so far for modelling the csv data in case we should lose connection or some other unavoidable disaster occurs. In the Menu, select **File -\\> Save**:\n\n\n ![](images/Picture25.png)\n\n Next, we will configure a model for the **CPU** data. From the **Model Design** window, right-click on **CPU** under the ITM data source, and select **New Metric Group**.\n\n ![](images/Picture26.png)\n\n Name the metric group **CpuUtilization**:\n\n ![](images/Picture27.png)\n\n Again, we see that the mediation tool did a good job of identifying which CSV columns should be used for the timestamp, resource, and even identified that there are two metric columns in the file:\n\n ![](images/Picture28.png)\n\n So next, define the timestamp format by selecting the **Model Properties** tab, opening up the **CpuUtilization** model definition, selecting **Timestamp**, and selecting **String** as the Data Type and entering **dd/MM/yy HH:mm** for the time format, ensuring to hit **Enter** when done typing in the time format.\n\n ![](images/Picture29.png)\n\n Return to the **Model Design** tab and test the model design by clicking on the **preview extraction** button.\n\n ![](images/Picture30.png)\n\nVerify data in extraction preview:\n\n ![](images/Picture31.png)\n\n Ensure that you have both metrics **User%** and **Wait%** in the preview.\n\n Again, save your model progress by clicking on\n\n **File -\\> Save**\n\n Next, create a metric group for database data. Right-click on \"DB\" and select \"New Metric Group\":\n\n ![](images/Picture32.png)\n\n\n Call your metric group **DatabasePerformance** and click **Ok**:\n\n ![](images/Picture33.png)\n\n As with the previous two groups, we again see that the mediation tool has done a good job of classifying which fields are the timestamp, resource, and metric.\n\n ![](images/Picture34.png)\n\n Define your timestamp format in Model Properties tab, making sure you select the timestamp definition under **DatabasePerformance**, setting it to **dd/MM/yy HH:mm**.\n\n ![](images/Picture35.png)\n\n Again, return to the Model Design tab and click on the extraction preview icon to preview the extraction:\n\n ![](images/Picture36.png)\n\n Save the model progress by selecting\n\n **File -\\> Save**\n\n\n Lastly, we will model the DISKBUSY csv. Right-click on DISKBUSY under the ITM data source, and select \"New Metric Group\":\n\n ![](images/Picture37.png)\n\n Call the metric group **DiskPerformance**\n\n ![](images/Picture38.png)\n\n\n Review the model definition:\n\n ![](images/Picture39.png)\n\n Note that in the CSV file, we have four columns: TIME, RESOURCE, DISKNAME, and DISKBUSY. The mediation tool did a good jo of defining the Timestamp and identifying the metric name. However, for the resource key, it only used the RESOURCE field in the CSV file. If we don't add the DISKNAME field to the resource key, the Metric Manager will just aggregate/average the data for all disks to a single metric (DISKBUSY) at the Node level, which is certainly not what we want.\n\n The resource key is important for metrics that may be specific to a component or set of components on a node or device. For example, if Metric Manager is receiving network utilization metrics from a server that has **2 network interface cards**, you need to define the resource not as simply the Node, but the **Node + network interface card name**… e.g. \"server1:eth0 and server1:eth1\".\n\nThe mediation tool does this for us when we define multiple columns as the **Resource Key**.\n\n In our case, what we need to do is include the **DISKNAME** as part of the Resource Key, so we don't just aggregate/average all values for all disks at the Node level. To do this, select **DISKNAME** from the ITM data tree with the left mouse button, hold down the button and drag it into the Attributes window to add DISKNAME as an attribute. Alternatively you can right-click on **DISKNAME** and select **Add Attribute**:\n\n ![](images/Picture40.png)\n\n You'll notice that it automatically gives it **Application** as an Attribute name. You can leave it as is or change it to **Disk** or something descriptive if you wish.\n\n Next, repeat the same process to add the DISKNAME field to the resource key. In the ITM data tree, select **DISKNAME** with your left mouse button, hold down the button and drag it to the Resource Key. Alternatively you can right-click on DISKNAME and select **Add Resource Key**:\n\n ![](images/Picture41.png)\n\n Finally, go to the **Model Properties** tab and define the timestamp format as **dd/MM/yy HH:mm**:\n\n ![](images/Picture42.png)\n\n Return to the **Model Design** tab and click on the **preview extraction** icon. A Warning window will pop up:\n\n ![](images/Picture43.png)\n\n\n What this is telling us is that there are multiple entries for values in each time interval defined in the file.\n\n This could be a data quality issue. Minimize the mediation utility and open up a new terminal window, and issue the following commands:\n\n```sh\ncd /home/scadmin/BookInfoDemo/data/bookinfo/data\ncat DISKBUSY__20201001-0000__20201002-0000.csv |head -n 20\n```\n\nYou'll notice that indeed there are multiple entries for a given time stamp.\n\n If this were a data quality issue, it may be due to the exclusion of another component from the csv file that would make the resource more unique, that would be needed to be included in the resource key. Or perhaps there is something not right with how the data is being extracted and written.\n\n In our case, upon further investigation and a discussion with the team that manages the tool that measures DIKSBUSY, we discover the DISKBUSY metric is measured and written every 10 seconds, but the time format as written has left off the seconds column. This isn't a problem in this case, because the smallest aggregation interval Metric Manager supports is 5 minutes, and these values will be all aggregated up to 5 minutes anyway… even if the seconds column were included. So in our case, we can safely ignore the error.\n\n Data quality is paramount to ensure maximum value from Metric Manager. Be aware of the format of the data, and if there is something you come across that doesn't seem right, consult the owners of the tool that's generating the data for Metric Manager and verify that it is correct.\n\n To continue the lab, return to the Mediation Tool window and click **Ok** and verify that your extraction preview shows data:\n\n ![](images/Picture44.png)\n\n\n Now that we have completed the data modelling, we are ready to \"Deploy\" our model to the BIGBLUE topic that we created earlier. Before doing that, select\n\n **File -\\> Save**\n\n To save the model:\n\n ![](images/Picture45.png)\n\n## 3-5: Deploying the model\n\n Next, to deploy the model, right-click on the BIGBLUE.pamodel title under the TECHJAM2022 project, and select **Deploy Model**:\n\n ![](images/Picture46.png)\n\n And change the following items in the configuration window:\n\n`JDBC URL: jdbc:db2://pi-template.Hybrid-Squad.cloud:50000/SCAPIDB`  \n`Password:` **<span style=\"color:green\"><LAB PASSWORD\\></span>**    \n`<CHECK \"Save as Project Defaults\">`  \n\n ![](images/lab3-addon1.png)\n\n Click **Ok** when finished:\n\n ![](images/Picture47.png)\n\n If there are multiple topics configured in the Metric Manager install, you will need to select the appropriate topic from the drop-down list. Select \"BIGBLUE\" and click \"Ok\":\n\n If there is just a single topic, it will deploy straight away.\n\n\n After the KPI count is complete, click **Yes** to deploy the model to the topic:\n\n ![](images/Picture48.png)\n\n\n After the deployment completes successfully, exit the mediation tool to return to the command line.\n\n\n## 3-6: **Ingesting CSV data for machine learning**\n\n\n### Start the topic\n\n Now that we have successfully created and deployed a model to our topic, we can extract the data for analysis and see how the Metric Manager learns the behavior of the data and identifies situations where the data has acted contrary to the learned behavior. We must first start the topic. Exit the mediation tool to return to the command line and run the following command to start the topic:\n\n```sh\n/opt/IBM/scanalytics/analytics/bin/start.sh -t=BIGBLUE\n```\n\n After about 2 minutes you should see a message indicating that the topic is running:\n\n_ **Instance AnalyticsBIGBLUE running** _\n\n If you see an error message, rather than the above message, try stopping the Streams instance, and then start the topic using the following commands:\n\n```sh\n/opt/IBM/scanalytics/analytics/bin/stop.sh -s\n/opt/IBM/scanalytics/analytics/bin/start.sh -t=BIGBLUE\n```\n\nIf you still have problems starting up the topic, ask a lab proctor for assistance.\n\n### Verify required services are running\n\n Before we start the extraction, verify that the Netcool services are up and running by issuing the following command, using the password **<span style=\"color:green\"><LAB PASSWORD\\></span>** as the password:\n\n```sh\nsudo systemctl status nco\n```\n\n The Active line should say active. If it is not showing an active status, start it up by running:\n\n```sh\nsudo systemctl stop nco\nsudo systemctl start nco\n```\n\n Next verify the Netcool GUI is up and running by issuing the following command:\n\n```sh\nsudo systemctl status netcoolwebgui\n```\n\n If the \"Active\" line is showing any status other than \"active (exited)\", start it up by running:\n\n```sh\nsudo systemctl stop netcoolwebgui\nsudo systemctl start netcoolwebgui\n```\n\n Ensure that the DB2 database is running by running:\n\n```sh\ndb2pd -\n```\n\nThe \"Database Member 0\" should show active. If it is not active, start DB2 by running:\n\n```sh\ndb2start\n```\n\n Finally, verify that the Metric Manager UI is running by issuing the following command:\n\n```sh\n/opt/IBM/scanalytics/UI/bin/pi.sh -status\n```\n\n If the IBM Websphere Liberty Profile is DOWN, run the following command to start it:\n\n```sh\n/opt/IBM/scanalytics/UI/bin/pi.sh -start\n```\n\nand verify the status again to ensure the profile is UP.\n\n### Run extraction\n\n Once the Netcool core, Web GUI, DB2, and Metric Manager UI components are running, we are ready to extract the data. Run the following command to start extraction:\n\n```sh\n$PI_HOME/bin/admin.sh run_extractor_instance -t=BIGBLUE -s=20201001-0000 -e=20201017-1200\n```\n\nThis will initiate the ingestion process, and start the extraction from October 1st, 2020 at midnight, and end the extraction when it has ingested data all the way up to October 17\n\n To monitor the ingestion,\n\n```sh\ncd ~/BookInfoDemo/data/bookinfo/data\nls\n```\n\n If you list the directory, you will see as the ingestion process proceeds, files that are being read are changed to prepend the filename to INUSE.\\<filename\\> a such:\n\n_INUSE.DISKBUSY\\_\\_20201002-0000\\_\\_20201003-0000.csv_\n\n Next, list the files in the 'good' directory. You will see files that have completed ingestion arrive in the 'good' directory.\n\n```sh\nls good\n```\n\n By watching both the data directory and the good directory, you will be able to see which files are being processed (INUSE), and which files are complete and end up in the 'good' directory.\n\n Take a break. In about 12 to 15 minutes, the data directory will be empty of all CSV files, which have been moved to the good directory. Machine learning and anomaly detection of the ingested data will complete. The next lab will walk through navigating the UI and viewing the results.\n\n\n","type":"Mdx","contentDigest":"aeb0ee1da5c51a32b603c753f8391d8f","owner":"gatsby-plugin-mdx","counter":924},"frontmatter":{"title":"Mediating CSV Metric Data","description":"This lab will illustrate how to analyze CSV file structure, how to create a Metric Manager topic model, how to deploy the model to a topic, and how to start ingestion of CSV data for machine learning"},"exports":{},"rawBody":"---\ntitle: Mediating CSV Metric Data\ndescription: This lab will illustrate how to analyze CSV file structure, how to create a Metric Manager topic model, how to deploy the model to a topic, and how to start ingestion of CSV data for machine learning\n---\n\n<AnchorLinks>\n  <AnchorLink>3-1: Lab Introduction</AnchorLink>\n  <AnchorLink>3-2: Creating a topic</AnchorLink>\n  <AnchorLink>3-3: Topic configuration</AnchorLink>\n  <AnchorLink>3-4: Creating a model to ingest CSV</AnchorLink>\n  <AnchorLink>3-5: Deploying the model</AnchorLink>\n  <AnchorLink>3-6: Ingesting CSV data for machine learning</AnchorLink>\n</AnchorLinks>\n\n## 3-1: **Introduction**\n\nThe Metric Manager can consume data in four primary ways: Modelling/Mediating data that is in CSV format, Modelling/Mediating data that resides in a database via JDBC connection, and ingesting pre-formed JSON through either the REST mediation service or Kafka.\n\nThis lab will be showing you how to consume data from CSV files. There are CSV files located in the _/home/scadmin/BookInfoDemo/data/bookinfo/data_ directory. These files contain metrics for Application response time (APP), CPU utilization (CPU), Database transaction volume (DB), and disk utilization (DISKBUSY). This data spans a time period of 17 days.\n\n We will create a Metric Manager topic to receive this data, create a model for this data with the Mediation Tool, deploy that model, then start the topic and start ingesting the data.\n\n## 3-2: **Creating a topic**\n\n\n  To create a topic, open a terminal window and run the following commands:\n\n```sh\ncd /opt/IBM/scanalytics/analytics/bin\n./admin.sh create_topic BIGBLUE\n```\n\n This will take a minute or two while the Metric Manager builds the topic tables in the DB2 database and creates the appropriate analytics/streams configuration.\n\n Once the **create\\_topic** command completes, you can list the topics that are configured on the system by issuing the following command. You should see your new BIGBLUE topic, along with the TEST topic that we created to verify the iFix install:\n\n```sh\n./admin.sh show_topics\nTEST\nBIGBLUE\n```\n\n You may delete the TEST topic you created earlier to verify that the update completed successfully if you wish.\n\n```sh\n./admin.sh delete_topic TEST\n```\n\n## 3-3: **Topic configuration**\n\n### Topic configuration considerations\n\n_Aggregation interval_\n\n There are considerations for topic configuration that need to be accounted for based on the data that the Metric Manager will be ingesting. One of the more important aspects of topic configurations is the system aggregation interval. The aggregation interval defines the rate, in minutes, of the metric data that Metric Manager will be consuming. This is largely dictated by the source of the metric data (e.g. an APM tool, network performance monitor, etc). The supported PI intervals are 5, 10, 15, and 60 minutes. If the source that is generating metrics for MM ingestion is collecting at 5 minutes, it is best to set the aggregation interval for the topic to 5. If the metrics are generated at 1 minute intervals, set the topic to 5 minutes, and PI will automatically aggregate the data to the 5 minute interval.\n\n For 5 minute data, time intervals in minutes/seconds are:\n\n 00:00 – 04:59  \n 05:00 – 09:59  \n 10:00 – 14:59  \n …  \n …  \n 50:00 – 54:59  \n 55:00 – 55:50  \n\n Likewise, 10 minute intervals are:\n\n 00:00 – 09:59  \n 10:00 – 19:59  \n …  \n …  \n 40:00 - 49:49  \n 50:00 – 59:59  \n\n Investigate the data that we will model and ingest. Change your directory to where our CSV files are located and list the contents of the directory:\n\n```sh\ncd /home/scadmin/BookInfoDemo/data/bookinfo/data\nls\n```\n\n_CSV file naming_\n\nYou'll note that there are a number of csv files in that directory that look to adhere to a standard naming convention. There are rules that you must follow for naming these files, and those rules are documented in the \"IBM Operations Analytics Predictive Insights\" documentation at this link:\n\n<a href=\"https://www.ibm.com/docs/en/oapi/1.3.6?topic=SSJQQ3\\_1.3.6/com.ibm.scapi.doc/admin\\_guide/r\\_tasp\\_csvrules.html\" target=\"_blank\">Rules for using CSV data sources</a>\n\n In short:\n\n - the file name must start with a group name, and contain a delimiter\n - next, the file name must contain the start date for the data residing in the csv file, and another delimiter\n - the end date is optional if the data that resides in the CSV file is for a single interval, but if data within the CSV file spans multiple intervals, it is required\n - the file name must end in .csv\n\n The files names that we have created for execution of this lab have the format:\n\n`<GROUP>__<STARTDATE>-<STARTTIME>__<ENDDATE>-<ENDTIME>.csv`\n\n You will note that some groups have individual daily files (CPU, Diskbusy), and others have one csv file for the entire span of data (APP, DB). The Metric Manager extraction process is smart enough to read data for each of its intervals from the files simply based on the timestamp of the files that exist, as well as the timestamps within the files. Since we will be ingesting files in 'backlog' historical mode, and not live, having daily files or a file that spans 17 days is fine. If we were ingesting live data for anomaly detection, Metric Manager would expect to have a single file that contains all metric data for each aggregation interval. In live ingestion, the extractor instance will pick up the latest file for the previous aggregation interval (e.g. 00:00-04:59) and process it.\n\n So in looking at the file names, we can see that the data in this directory starts at October 1st at midnight, 2020, and ends on October 17th, noon, or about 17 days. Next, let's look at the interval of the data contained in the files.\n\n_Resources within Metric Manager_\n\nAny data sent into Metric Manager (whether it be CSV, or REST/Kafka) must contain enough information to properly classify the component that the metric is related to. For example, you may be collecting metrics for a router. That router may have multiple interfaces. There are metrics that are applicable to the router **node** itself (e.g. CPU utilization, buffer utilization), and there are metrics that are applicable to an **interface** on that router (e.g. packet drops, in bits per second, out bits per second, etc). \n\n Run the following command to page through the data contained in the APP file:\n\n```sh\nmore APP__20201001-0000__20201017-1200.csv\n```\n\nNote that the file has the required CSV header, which must be the first entry of the CSV file:\n\n**_Time,RESOURCE,ResponseTime_**\n\n In looking at the file, it's clear that the timestamp is the first column, and looking at the times they are timestamped at 5 minute intervals. Look at the other files and confirm that they are also in 5 minute intervals with the exception of DISKBUSY, which appear to be in minute intervals. That is fine as MM will aggregate our data for us (more on this later).\n\n Also note that the timestamps are in a different format than the file name. It is in the format \"Day/Month/Year Hour:Minute\", using literal slashes, space, and colon as delimiters. Additionally the Year is in 2 digit format, rather than four. So during our model creation, we must use the notation \"dd/MM/yy HH:mm\". It is important to be cognizant of timestamp formats whether they be in the file name, or within the CSV file itself. They may not always be the same, and we will take this into account when we create our model for the CSV data.\n\n### Configuring topic options\n\n Given the velocity of the data contained in these CSV files, we will need to configure the Metric Manager topic aggregation interval to 5 minutes. To set the aggregation interval, first check to see what it is currently set to:\n\n```sh\ncd /opt/IBM/scanalytics/analytics/bin\n./admin.sh show -t=BIGBLUE\n```\n\nThis will show all of the configurable options for the BIGBLUE topic. The aggregation interval is \"system.aggregation.interval\". You can see it is currently set to 15 minutes. To change it to 5 minutes, issue the following command:\n\n```sh\n./admin.sh set -t=BIGBLUE system.aggregation.interval 5\n```\n\nTo verify the change, run the following command:\n\n```sh\n./admin.sh show -t=BIGBLUE |grep system.aggregation.interval\n```\n\n... you should see that the system.aggregation.interval property is 5 minutes:\n\n`system.aggregation.interval: 5`\n\n\nWhen ingesting historical data, it's important to configure the topic such that it doesn't automatically delete the data you ingest. Metric Manager is not meant to be used as a long-term metric data repository... it is meant to keep the data long enough to successfully identify changes in behavior, and to keep a small amount of historical for short-term review. By default, a topic's metric data is cleared out after 15 days, while anomaly alarm data is kept for 180 days. Since we have data originating from 2020, we need to set a couple more topic parameters to ensure the system doesn't age out our data. Set the following options for the topic BIGBLUE:\n\n```sh\n./admin.sh set -t=BIGBLUE sta.corr.retention.days 1095\n./admin.sh set -t=BIGBLUE system.alarm.history.retention.days 1095\n./admin.sh set -t=BIGBLUE system.metric.retention.days 1095\n```\n\nAnother thing you may need to do during a proof of concept is disable alarm clearing. This is required to be disabled if you are analyzing past data and need to demonstrate what the anomaly alerts that arrive in Event Manager look like, without having them clear when the anomaly occurrence stops. Since we'll be viewing Event Manager events as part of our labs, we'll disable it:\n\n```sh\n./admin.sh set -t=BIGBLUE system.alarm.autoclear false\n```\n\n Finally, double-check and verify the topic configuration by running the show topic command:\n\n```sh\n./admin.sh show -t=BIGBLUE\n```\n\n ![](images/Picture1.png)\n\n\n Topic configuration is complete!\n\n## 3-4: **Creating a model to ingest CSV**\n\n The Mediation Tool is the component you will use to model the data contained within the CSV files so that Metric Manager can ingest, process, learn, and identify anomalies within the data. The tool allows you to create a 'model' of the structure of the CSV files you'll be working with, from within a user-friendly Eclipse interface. You can start the Mediation Tool by issuing the following commands from your VNC session terminal:\n\n```sh\ncd /opt/IBM/scanalytics/mediationtool/eclipse\n./eclipse\n```\n\n You will be presented with the initial config screen. Leave the default and click \"Ok\".\n\n ![](images/Picture2.png)\n\n\n You will be presented with the main window. In this step we will create a new Predictive Insights project. A project is simply a folder that contains one or more model definitions. From the main window select **File-\\>New-\\>Project** from the menu bar:\n\n ![](images/Picture3.png)\n\n In the next screen, open the **Predictive Insights Wizards** tree, select **Predictive Insights Project**, and click **Next**:\n\n ![](images/Picture4.png)\n\n The next screen will prompt you for a project name. Enter **TECHJAM2022** and click the **Finish** button.\n\n ![](images/Picture5.png)\n\n Next, you will be asked to switch the perspective to Predictive Insights. Click **Yes**.\n\n ![](images/Picture6.png)\n\n Next, close the welcome screen by clicking on the X in the tab above the text **Welcome to Eclipse**:\n\n ![](images/Picture7.png)\n\n Finally, you will be in your modelling view. Maximize the window to take up the full screen, you will need the real estate:\n\n ![](images/Picture8.png)\n\n### Create a data source\n\n  Next, we will create a data source. Right click on your **TECHJAM2022** project in the navigator, and select **New -\\> Predictive Insights Data Source**\n\n ![](images/Picture9.png)\n\n In the dialog window that appears, change the **Filename** parameter to **BIGBLUE.pamodel** and click **Next**.\n\n ![](images/Picture10.png)\n\n In the next screen, you will select **File System** from the drop-down box, and select **Metric names in column headers**.\n\n ![](images/Picture11.png)\n\n Click **Finish**. You now have a data source that we will configure to ingest our csv files.\n\n_A note about CSV structure: CSV files can be in either **wide** or **skinny** format. **Wide** format specifies the metric name in the CSV header. For example, in our data the CPU group has the following header:\n\n `\"TIME\",\"RESOURCE\",\"User%\",\"Wait%\"`  \n `01/10/20 00:00,\"details\",84.9,0`  \n `…`  \n `…`  \n\n Above we can see there are two metric definitions in the csv header, \"User%\" and \"Wait%\". That indicates that a single CSV line contains multiple metric definitions per resource. Hence, we selected **Metric names in column headers**.\n\n You may come across a situation where the metric name appears in the csv file data rather than the header. Compare the previous example with the following:\n\n `\"TIME\",\"RESOURCE\",\"METRICNAME\"`  \n `01/10/20 03:20,details,User%,84.9`  \n `01/10/20 03:20,details,Wait%,0`  \n `…`  \n `…`  \n\n The CSV contains a generic \"METRICNAME\" indicator, and the various metric name appears within the csv lines, rather than the header. This is **skinny** format, and each line contains only one metric. When you model the CSV file, and select the metric column, it will show all of the available unique metric names as available, and you can include/exclude metrics as needed.\n\n_Note: If you need to ingest files in both skinny and wide format, you must create separate data sources for each type, as you cannot match skinny and wide formats in a single project. You can deploy multiple CSV data sources to a single topic, but they must be done as the same deployment action – that is you must multi-select the data sources, then select \"deploy model\"._\n\n### Configure the filesystem details.\n\n The file system details define where the files are located and the structure and date format of the file names. \n\n You will be at the following screen:\n\n ![](images/Picture12.png)\n\n First, under **File System Details**, change the **File Path** to:\n\n**/home/scadmin/BookInfoDemo/data/bookinfo/data**\n\n This tells the mediation tool where to expect the CSV files.\n\n Next, modify the name pattern. As we learned previously, a CSV file name must start with a **group reference**, followed by a **delimiter**, followed next by a **start time** and  an **end time** (with the end time only strictly necessary when the file contains data for a span of multiple intervals). When defining the name pattern, we use regular expressions to match. To keep it simple, use the following regex to define the name pattern:\n\n**(.\\*)\\_\\_(.\\*)\\_\\_(.\\*)\\\\.csv**\n\n\n The mediation tool, as well as the Metric Manager extractor, uses the first regex match as the metric group identifier, the second as the Start Time, and, if defined, the third regex match as an end time.\n\n Next we will change the time format. The time format uses standard Java time format notation. Looking at our CSV file timestamps, we see that they are formatted as such:\n\n 20201001-0000\n\n Which, in Java time format notation, is:\n\n yyyyMMdd-HHmm\n\n As such, change the time format to \"yyyyMMdd-HHmm\"\n\nThe files provided are in GMT already so we can leave the timezone as it is.\n\n _Note: It is important to be cognizant of the timezone of the data and configure this appropriately, so that the analytics can properly learn the differences between weekday and weekend behavior patterns, as well as proper correlation of metrics across all CSV files. In many cases the timezone of the data you work with will be GMT, but you may run into cases where they are provided in some local time zone. Make sure that you verify with the customer what timezone the data timestamps are in._\n\n Once all the File System Details options have been configured, click the \"Test Matching\". You should get green checkmarks for the files in our directory indicating that the name pattern and time format worked properly:\n\n ![](images/Picture13.png)\n\n If you see errors indicating 'pattern mismatch', or do not see the green checkmarks, double-check your configuration and try again. If you sill have difficulty consult a lab proctor.\n\n\n### Defining the model from the data\n\n The next step is to actually build the model from the CSV file structures. Below the **File System Details** window that we just finished configuring and testing, you will see three tabs. Select the **Model Design** tab and click **Ok** in the dialog window that pops up.\n\n ![](images/Picture14.png)\n\n\n The model design window shows the configured data source (ITM) that we just finished configuring. Right-click on **ITM**, and select **Synchronize Schema**. This will cause the mediation tool to examine the structure and contents of the CSV files that match the pattern we defined in the previous section:\n\n ![](images/Picture15.png)\n\n\n The mediation tool will then churn through the available files in our defined directory, analyze the CSV structure, and present us with the metric groups it found in our directory. In the preview window, select **Add New Elements**, and it will select all of the CSV file structures it found in that directory.\n\n ![](images/Picture16.png)\n\n select all of the tables and click **Ok**.\n\n Next, for our ITM data source, open up the tree view that is now available:\n\n ![](images/Picture17.png)\n\n Expanding the tree view **fully** shows all of the available tables and columns in our CSV files.:\n\n ![](images/Picture18.png)\n\n Next, we will create metric groups for each of these. Right-click on **APP**, and select **New metric group**:\n\n ![](images/Picture19.png)\n\n\n You have the option of naming the metric group to anything that is descriptive of the type of data in the respective group. In our case, we'll call it **ApplicationPerformance**, and click **Ok**:\n\n ![](images/Picture20.png)\n\n The next screen will show an attempt by the modelling tool to create a model based on the composition of the CSV header:\n\n ![](images/Picture21.png)\n\n The mediation tool did a good job of providing a model based on the structure for this instance. It selected the proper CSV column for the **Timestamp (Time)**, the proper CSV column for the **Resource Key (RESOURCE)**, and the proper column for the **Metric Name (ResponseTime)**. So in this case, there isn't any additional work we need to do to define the model. But we do need to configure the timestamp format (as we did in the File Details definition). To do this, click select the **Model Properties** tab below the **Model Design** window. Open up the **Model -\\> ApplicationPerformance -\\> Timestamp** entry, and change the **Data Type** to **String** if it isn't already.\n\n Recall our earlier examination of the CSV file content. We discovered that the time format of the timestamp within the CSV file is different that it is in the filename. As such we must define **Time Format** as **dd/MM/yy HH:mm** (don't forget the space between the date and time). When you are finished entering the Time Format field, hit the **enter** key so that the entry is saved.\n\n ![](images/Picture22.png)\n\n Finally, click on the **Model Design** tab to return to the model design window, and click on the **preview data** icon to verify that the model is working:\n\n ![](images/Picture23.png)\n\n You should see a preview of the data extraction in the **Data Extraction Preview** tab:\n\n ![](images/Picture24.png)\n\n If you do not see data in the preview after clicking on the **Preview extraction** icon, verify the configuration of your timestamp in the model properties tab. If you feel you have verified everything and are still having issues, consult a lab proctor.\n\n Finally, take a moment to save the work we have done so far for modelling the csv data in case we should lose connection or some other unavoidable disaster occurs. In the Menu, select **File -\\> Save**:\n\n\n ![](images/Picture25.png)\n\n Next, we will configure a model for the **CPU** data. From the **Model Design** window, right-click on **CPU** under the ITM data source, and select **New Metric Group**.\n\n ![](images/Picture26.png)\n\n Name the metric group **CpuUtilization**:\n\n ![](images/Picture27.png)\n\n Again, we see that the mediation tool did a good job of identifying which CSV columns should be used for the timestamp, resource, and even identified that there are two metric columns in the file:\n\n ![](images/Picture28.png)\n\n So next, define the timestamp format by selecting the **Model Properties** tab, opening up the **CpuUtilization** model definition, selecting **Timestamp**, and selecting **String** as the Data Type and entering **dd/MM/yy HH:mm** for the time format, ensuring to hit **Enter** when done typing in the time format.\n\n ![](images/Picture29.png)\n\n Return to the **Model Design** tab and test the model design by clicking on the **preview extraction** button.\n\n ![](images/Picture30.png)\n\nVerify data in extraction preview:\n\n ![](images/Picture31.png)\n\n Ensure that you have both metrics **User%** and **Wait%** in the preview.\n\n Again, save your model progress by clicking on\n\n **File -\\> Save**\n\n Next, create a metric group for database data. Right-click on \"DB\" and select \"New Metric Group\":\n\n ![](images/Picture32.png)\n\n\n Call your metric group **DatabasePerformance** and click **Ok**:\n\n ![](images/Picture33.png)\n\n As with the previous two groups, we again see that the mediation tool has done a good job of classifying which fields are the timestamp, resource, and metric.\n\n ![](images/Picture34.png)\n\n Define your timestamp format in Model Properties tab, making sure you select the timestamp definition under **DatabasePerformance**, setting it to **dd/MM/yy HH:mm**.\n\n ![](images/Picture35.png)\n\n Again, return to the Model Design tab and click on the extraction preview icon to preview the extraction:\n\n ![](images/Picture36.png)\n\n Save the model progress by selecting\n\n **File -\\> Save**\n\n\n Lastly, we will model the DISKBUSY csv. Right-click on DISKBUSY under the ITM data source, and select \"New Metric Group\":\n\n ![](images/Picture37.png)\n\n Call the metric group **DiskPerformance**\n\n ![](images/Picture38.png)\n\n\n Review the model definition:\n\n ![](images/Picture39.png)\n\n Note that in the CSV file, we have four columns: TIME, RESOURCE, DISKNAME, and DISKBUSY. The mediation tool did a good jo of defining the Timestamp and identifying the metric name. However, for the resource key, it only used the RESOURCE field in the CSV file. If we don't add the DISKNAME field to the resource key, the Metric Manager will just aggregate/average the data for all disks to a single metric (DISKBUSY) at the Node level, which is certainly not what we want.\n\n The resource key is important for metrics that may be specific to a component or set of components on a node or device. For example, if Metric Manager is receiving network utilization metrics from a server that has **2 network interface cards**, you need to define the resource not as simply the Node, but the **Node + network interface card name**… e.g. \"server1:eth0 and server1:eth1\".\n\nThe mediation tool does this for us when we define multiple columns as the **Resource Key**.\n\n In our case, what we need to do is include the **DISKNAME** as part of the Resource Key, so we don't just aggregate/average all values for all disks at the Node level. To do this, select **DISKNAME** from the ITM data tree with the left mouse button, hold down the button and drag it into the Attributes window to add DISKNAME as an attribute. Alternatively you can right-click on **DISKNAME** and select **Add Attribute**:\n\n ![](images/Picture40.png)\n\n You'll notice that it automatically gives it **Application** as an Attribute name. You can leave it as is or change it to **Disk** or something descriptive if you wish.\n\n Next, repeat the same process to add the DISKNAME field to the resource key. In the ITM data tree, select **DISKNAME** with your left mouse button, hold down the button and drag it to the Resource Key. Alternatively you can right-click on DISKNAME and select **Add Resource Key**:\n\n ![](images/Picture41.png)\n\n Finally, go to the **Model Properties** tab and define the timestamp format as **dd/MM/yy HH:mm**:\n\n ![](images/Picture42.png)\n\n Return to the **Model Design** tab and click on the **preview extraction** icon. A Warning window will pop up:\n\n ![](images/Picture43.png)\n\n\n What this is telling us is that there are multiple entries for values in each time interval defined in the file.\n\n This could be a data quality issue. Minimize the mediation utility and open up a new terminal window, and issue the following commands:\n\n```sh\ncd /home/scadmin/BookInfoDemo/data/bookinfo/data\ncat DISKBUSY__20201001-0000__20201002-0000.csv |head -n 20\n```\n\nYou'll notice that indeed there are multiple entries for a given time stamp.\n\n If this were a data quality issue, it may be due to the exclusion of another component from the csv file that would make the resource more unique, that would be needed to be included in the resource key. Or perhaps there is something not right with how the data is being extracted and written.\n\n In our case, upon further investigation and a discussion with the team that manages the tool that measures DIKSBUSY, we discover the DISKBUSY metric is measured and written every 10 seconds, but the time format as written has left off the seconds column. This isn't a problem in this case, because the smallest aggregation interval Metric Manager supports is 5 minutes, and these values will be all aggregated up to 5 minutes anyway… even if the seconds column were included. So in our case, we can safely ignore the error.\n\n Data quality is paramount to ensure maximum value from Metric Manager. Be aware of the format of the data, and if there is something you come across that doesn't seem right, consult the owners of the tool that's generating the data for Metric Manager and verify that it is correct.\n\n To continue the lab, return to the Mediation Tool window and click **Ok** and verify that your extraction preview shows data:\n\n ![](images/Picture44.png)\n\n\n Now that we have completed the data modelling, we are ready to \"Deploy\" our model to the BIGBLUE topic that we created earlier. Before doing that, select\n\n **File -\\> Save**\n\n To save the model:\n\n ![](images/Picture45.png)\n\n## 3-5: Deploying the model\n\n Next, to deploy the model, right-click on the BIGBLUE.pamodel title under the TECHJAM2022 project, and select **Deploy Model**:\n\n ![](images/Picture46.png)\n\n And change the following items in the configuration window:\n\n`JDBC URL: jdbc:db2://pi-template.Hybrid-Squad.cloud:50000/SCAPIDB`  \n`Password:` **<span style=\"color:green\"><LAB PASSWORD\\></span>**    \n`<CHECK \"Save as Project Defaults\">`  \n\n ![](images/lab3-addon1.png)\n\n Click **Ok** when finished:\n\n ![](images/Picture47.png)\n\n If there are multiple topics configured in the Metric Manager install, you will need to select the appropriate topic from the drop-down list. Select \"BIGBLUE\" and click \"Ok\":\n\n If there is just a single topic, it will deploy straight away.\n\n\n After the KPI count is complete, click **Yes** to deploy the model to the topic:\n\n ![](images/Picture48.png)\n\n\n After the deployment completes successfully, exit the mediation tool to return to the command line.\n\n\n## 3-6: **Ingesting CSV data for machine learning**\n\n\n### Start the topic\n\n Now that we have successfully created and deployed a model to our topic, we can extract the data for analysis and see how the Metric Manager learns the behavior of the data and identifies situations where the data has acted contrary to the learned behavior. We must first start the topic. Exit the mediation tool to return to the command line and run the following command to start the topic:\n\n```sh\n/opt/IBM/scanalytics/analytics/bin/start.sh -t=BIGBLUE\n```\n\n After about 2 minutes you should see a message indicating that the topic is running:\n\n_ **Instance AnalyticsBIGBLUE running** _\n\n If you see an error message, rather than the above message, try stopping the Streams instance, and then start the topic using the following commands:\n\n```sh\n/opt/IBM/scanalytics/analytics/bin/stop.sh -s\n/opt/IBM/scanalytics/analytics/bin/start.sh -t=BIGBLUE\n```\n\nIf you still have problems starting up the topic, ask a lab proctor for assistance.\n\n### Verify required services are running\n\n Before we start the extraction, verify that the Netcool services are up and running by issuing the following command, using the password **<span style=\"color:green\"><LAB PASSWORD\\></span>** as the password:\n\n```sh\nsudo systemctl status nco\n```\n\n The Active line should say active. If it is not showing an active status, start it up by running:\n\n```sh\nsudo systemctl stop nco\nsudo systemctl start nco\n```\n\n Next verify the Netcool GUI is up and running by issuing the following command:\n\n```sh\nsudo systemctl status netcoolwebgui\n```\n\n If the \"Active\" line is showing any status other than \"active (exited)\", start it up by running:\n\n```sh\nsudo systemctl stop netcoolwebgui\nsudo systemctl start netcoolwebgui\n```\n\n Ensure that the DB2 database is running by running:\n\n```sh\ndb2pd -\n```\n\nThe \"Database Member 0\" should show active. If it is not active, start DB2 by running:\n\n```sh\ndb2start\n```\n\n Finally, verify that the Metric Manager UI is running by issuing the following command:\n\n```sh\n/opt/IBM/scanalytics/UI/bin/pi.sh -status\n```\n\n If the IBM Websphere Liberty Profile is DOWN, run the following command to start it:\n\n```sh\n/opt/IBM/scanalytics/UI/bin/pi.sh -start\n```\n\nand verify the status again to ensure the profile is UP.\n\n### Run extraction\n\n Once the Netcool core, Web GUI, DB2, and Metric Manager UI components are running, we are ready to extract the data. Run the following command to start extraction:\n\n```sh\n$PI_HOME/bin/admin.sh run_extractor_instance -t=BIGBLUE -s=20201001-0000 -e=20201017-1200\n```\n\nThis will initiate the ingestion process, and start the extraction from October 1st, 2020 at midnight, and end the extraction when it has ingested data all the way up to October 17\n\n To monitor the ingestion,\n\n```sh\ncd ~/BookInfoDemo/data/bookinfo/data\nls\n```\n\n If you list the directory, you will see as the ingestion process proceeds, files that are being read are changed to prepend the filename to INUSE.\\<filename\\> a such:\n\n_INUSE.DISKBUSY\\_\\_20201002-0000\\_\\_20201003-0000.csv_\n\n Next, list the files in the 'good' directory. You will see files that have completed ingestion arrive in the 'good' directory.\n\n```sh\nls good\n```\n\n By watching both the data directory and the good directory, you will be able to see which files are being processed (INUSE), and which files are complete and end up in the 'good' directory.\n\n Take a break. In about 12 to 15 minutes, the data directory will be empty of all CSV files, which have been moved to the good directory. Machine learning and anomaly detection of the ingested data will complete. The next lab will walk through navigating the UI and viewing the results.\n\n\n","fileAbsolutePath":"/home/jason/workspace/IBM/waiops-tech-jam/src/pages/tutorials/mm/mm-mediating-csv/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}