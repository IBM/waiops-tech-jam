{"componentChunkName":"component---src-pages-tutorials-mm-mm-analyzing-results-index-mdx","path":"/tutorials/mm/mm-analyzing-results/","result":{"pageContext":{"frontmatter":{"title":"Reviewing Ingestion Results","description":"This lab will walk you through the steps to analyze and review the results of metric ingestion and machine learning"},"relativePagePath":"/tutorials/mm/mm-analyzing-results/index.mdx","titleType":"page","MdxNode":{"id":"1d766ab7-d71f-53ef-a627-351ff0a50d91","children":[],"parent":"c33d5622-7099-5dad-b667-76803cc374d5","internal":{"content":"---\r\ntitle: Reviewing Ingestion Results\r\ndescription: This lab will walk you through the steps to analyze and review the results of metric ingestion and machine learning \r\n---\r\n\r\n<AnchorLinks>\r\n  <AnchorLink>5-1: Lab Introduction</AnchorLink>\r\n  <AnchorLink>5-2: Accessing the dashboard</AnchorLink>\r\n  <AnchorLink>5-3: The anomaly search view</AnchorLink>\r\n  <AnchorLink>5-4: Launching anomaly details</AnchorLink>\r\n  <AnchorLink>5-5: Viewing alarms in Event Manager</AnchorLink>\r\n  <AnchorLink>5-6: Metric search</AnchorLink>\r\n  <AnchorLink>5-7: Anomaly distribution dashboard</AnchorLink>\r\n</AnchorLinks>\r\n\r\n\r\n## 5-1: Lab Introduction\r\n\r\nIn our previous lab, we configured a Metric Manager topic to receive data, we built a data model based on CSV files, we deployed the model to our topic, and we ran extraction to allow the Metric Manager to ingest the data and learn the behavior. Now that we have run our data through the system for analysis and machine learning, we will review the results of the metric analysis.\r\n\r\n## 5-2: Accessing the dashboard\r\n\r\n\r\nTo view the results of the ingestion and analysis, open Firefox from the Desktop menu:\r\n\r\n ![](images/lab4-Picture49.png)\r\n\r\nLogin as ncoadmin / **<span style=\"color:green\"><LAB PASSWORD\\></span>**:\r\n\r\n ![](images/lab4-Picture50.png)\r\n\r\n (note, if you don't see the above login screen when opening up Firefox, click on the \"IBM Dashboard Application Services Hub\" bookmark in the bookmarks bar).\r\n\r\n## 5-3: The anomaly search view\r\n\r\n From the main Dashboard window, find the \"Snowflake\" icon in the left menu pane, click on it, and select \"Service Diagnosis Anomaly Search\":\r\n\r\n ![](images/lab4-Picture51.png)\r\n\r\n You will be presented with the anomaly search screen, which defaults to the latest anomalies that exist across all topics in the Metric Manager instance. Note that at the top you can do arbitrary date range searches by selecting a start date/time and an end/date time:\r\n\r\n ![](images/lab4-Picture52.png)\r\n\r\n You can also filter down the anomaly list by entering a resource name, with the UI doing a live search for resources as you type. Try this by typing in \"details\" in the \"resource\" box:\r\n\r\n ![](images/lab4-add1.png)\r\n\r\n Take note of how it auto-fills the resource information per the resources that are in the system. In this case, we have two resources:\r\n\r\n -\"details\" which would likely have \"node-level\", metrics (aggregate CPU utilization, for example).\r\n -\"details:DB\\_SAN1\" is a more specific component of the node \"details\", and indicates metrics for that particular sub-component (in this case, the disk DB\\_SAN1).\r\n\r\n You can also filter by metric name as well.\r\n\r\n Next, let's take a look at the anomalies that were generated. Click on the \"Show Current Anomalies\" icon. This will bring you back to the last set of anomalies that exist in the system:\r\n\r\n ![](images/lab4-add2.png)\r\n\r\n Notice the anomaly that indicates there was an incident on 3 metrics across two nodes. This is a grouping of related anomalies. Open up the tree view to expose all of the multi-level, grouped anomaly alerts:\r\n\r\n ![](images/lab4-Picture53.png)\r\n\r\n## 5-4: Launching anomaly details\r\n\r\n Next, find the anomaly that indicates **ResponseTime is Higher than expected** for the **productpage-v1** resource, and select **Launch** (may be in a different location that what is shown below):\r\n\r\n ![](images/lab4-Picture54.png)\r\n\r\n\r\n This will launch you into the anomaly diagnosis screen for the Response Time anomaly:\r\n\r\n ![](images/lab4-Picture55.png)\r\n\r\n Here, you can see the general pattern of the metric, and the training period leading up to our anomaly. By default, the robust bounds algorithm training period, which provides the high/low bounds of a given metric, is set to 28 days. You do have control over the training period for each of the algorithms employed in Metric Manager. In MAD, the default training period is 14 days, and we'll see how that differs in the MAD lab, where we catch each of the three spikes seen above in the 28-day training period.\r\n\r\n For this view, let's zoom into the latest time leading up to this anomaly. You can do this by hovering your mouse over the graph at a time previous to the red area… hold down your left mouse button, then drag right to capture the time period, then release the left mouse button. When you release the left mouse button, it will change the graph into a zoomed view:\r\n\r\n\r\n ![](images/lab4-Picture56.png)\r\n\r\n Here we can hover over the first box of the red area and see the exact time that this metric went out of its learned baseline. We see the anomalous area in red, with the actual values of the metrics continuing up.\r\n\r\n Note the light pink section of the box. That area indicates the \"grace period\" before we generate an alert to Watson AIOps Event Manager. By default, if any **three** of **six** consecutive data points of a resource/metric combination are found to be anomalous, then Metric Manager will generate an alert, and continue to generate those alerts for each consecutive anomalous data point. Anomalies that don't reach this threshold are still generated, but are **suppressed**. They can still be viewed in the anomaly search window by selecting \"Include suppressed alarms\" but these are not forwarded to the Event Manager. This behaviour is configurable by modifying the following pair of topic configuration items:\r\n\r\n`nofm.error.count.min.size: 3`  - the number of anomalies within...\r\n`nofm.error.count.window.size: 6`  - any X consecutive time intervals\r\n\r\nFor 5-minute interval data, using the default configuration, that means that the earliest we would see an anomaly alert is within 15 minutes of the resource/metric going anomalous (e.g. 3 consecutive anomalous periods), and the longest we might see an alert is 30 minutes (e.g. first data point is anomalous, followed by 3 normal data points, then followed by 2 more anomalous data points, as an example). The properties could both be set to 1, which would result in immediate alarms for all detected anomalies.\r\n\r\n\r\nAt the right-hand side of the graph, you can see a dotted line illustrating the general trend and forecast of the anomaly. \r\n\r\n At the bottom of this view, you can see a list of metrics, with Response time selected. Crucially, there are additional related metrics that are contributing to this Response time issue, and they are included as **Related Metrics**. In the bottom section of the window, add the related metrics to the view by clicking on the respective check boxes:\r\n\r\n ![](images/lab4-Picture57.png)\r\n\r\nThe graph will build out to include the related metrics, to give you a full picture of the situation.\r\n\r\nHover your mouse over the graph, and note that, for any point in time on the graph, you can see the metric names, values, and time:\r\n\r\n ![](images/lab4-Picture58.png)\r\n\r\nIn the \"Related Metrics\" section below, you see six columns: Metric, Resource, Anomalous, Actual/Expected Value, Info, and Baseline.\r\n\r\n The Metric and Resource columns indicate the metric name and the resource associated with that metric that is anomalous.\r\n\r\n The **Anomalous** icon tells you which algorithm has deemed this particular resource/metric combination anomalous. Using your mouse, hover over the **Anomalous** icon for the **ResponseTime** entry for the **productpage-v1** resource. Note that the hover text notes **ResponseTime has gone outside of its normal expected value**. This indicates that the Robust Bounds algorithm has deemed this metric to be anomalous. That fact is also evidenced by the graph, where we see the baseline of the metric, and when it went outside of the baseline.\r\n\r\n ![](images/lab4-Picture59.png)\r\n\r\n\r\n In contrast, hover over the **Anomalous** icon for the **TransactionsPerInterval** for the **details** resource. Note the hover text indicating that the **TransactionsPerInterval** metric for the details database has flat-lined … that is, where before it was varying, now it is no longer varying. This is an algorithm that identifies an unusual change in behaviour that would very likely describe a problematic situation. In this case, the number of database transactions have not been able to exceed 410, where normally we would see it around 2500.\r\n\r\n ![](images/lab4-Picture60.png)\r\n\r\n\r\n To the right of the Anomalous column is the **Actual/Expected Value**. Looking at the response time metric for the productpage-v1 application, we see its actual value is just over five times slower than expected for this time of day (6638/1290).\r\n\r\nThe **Info** column provides information with regards to why a metric has been included in the group of metrics. For example, hover over the blue information icon for the \"TransactionsPerInterval\" metric. It is indicating that there are two reasons why this metric was included:\r\n\r\n**When ResponseTime on productpage-v1 is anomalous, 100% of the time TransactionsPerInterval on details is also anomalous** They are very strongly correlated from an anomaly occurrence prespective.\r\n\r\n**This metric is causally related to the target metric you launched**. This indicates that the behavior of these two metrics (productpage-v1/ResponseTime, which is the metric we launched to, and details/TransactionsPerInterval) tend to act in concert.\r\n\r\n ![](images/lab4-Picture61.png)\r\n\r\n Looking at the overall related anomalies gives cross-silo visibility that would otherwise not be possible by only looking at the specific monitoring tools that are collecting this data. We have brought together performance data from our APM solution, which provides response time information for the productpage-v1 application. We have brought in database transaction information from our database monitoring tool. And we have brought in disk performance metrics from our infrastructure monitoring tool. In seeing these anomalies together, we can surmise that the disk saturation is limiting the number of database transactions, which is, in turn, causing a serious performance problem for the application.\r\n\r\n## 5-5: Viewing alarms in Event Manager\r\n\r\nNext, we will look at the alarms in Event Manager that were generated for these anomalies. Anomaly alerts are generated through the use of a stdin probe that is installed and configured during the installation of the Metric Manager. There are default views and filters that are also installed into DASH, and a pre-configure event list that shows metric related events. To access this event list, click on the flag icon in the left pane, then select **Detected Anomalies** :\r\n\r\n ![](images/lab4-Picture62.png)\r\n\r\n\r\n At the top of this list, you can see a number of alerts that indicate general progress of the topic, including when the topic was started, when the extractor was started/stopped, when we started receiving data, the amount of data received required for training, when the model training started, and when the model was produced:\r\n\r\n ![](images/lab4-Picture63.png)\r\n\r\nThe anomaly alerts are shown below, including \"summarized\" anomaly alerts. Right-click on the alert indicating \"Incident on 3 metrics across 2 nodes\", and select the \"ServiceDiagnosis\"\r\n\r\n ![](images/lab4-Picture64.png)\r\n\r\n Much like from the Anomaly Search window, you are launched into the anomaly graph:\r\n\r\n ![](images/lab4-Picture65.png)\r\n\r\nOne thing that's a little bit different than looking at the anomaly search, is that you must click on the \"View Child Alarms\" tool to see anomaly events that make up the the incident:\r\n\r\n ![](images/lab4-Picture66.png)\r\n\r\n..which brings us the child alarms. You may need to again run the tool to see sub-groups of metrics as well:\r\n\r\n ![](images/lab4-Picture67.png)\r\n\r\n## 5-6: Metric search\r\n\r\nYou can perform aribitrary searches for resource / metric combinations. \r\n\r\nClick on the \"Snowflake\" icon in the left menu, and select **Service Diagnosis Metric Search**.\r\n\r\n ![](images/lab4-add3.png)\r\n\r\nThe Metric Search screen appears, and here you can search for an arbitrary resource / metric combination by starting with the resource name (in this case search for **productpage-v1**), then the metric group (ApplicationPerformance), and finally the metric name (ResponseTime). When you have the metric of interest, click on \"Add\" to add the metric to the graph:\r\n\r\n ![](images/lab4-add4.png)\r\n\r\ngives way to...\r\n\r\n\r\n ![](images/lab4-add5.png)\r\n\r\n\r\nSearching for aribitrary metrics allows you to view the behaviour of metrics **without** them necessarily being anomalous, which can come in handy if you want to ensure that a given metric is being collected, and also in situations where someone needs evidence that a metric or set of metrics of interest are acting within their learned baselines.\r\n\r\n## 5-7: Anomaly distribution dashboard\r\n\r\nFinally, there is also an anomaly distribution dashboard that shows the time frame of anomalies received, the number of anomalies that occurred for each time frame in a bar graph, and top 10 anomalous nodes, resources, and metrics. Its more useful for large datasets to be able to identify the when, over time, the most anomalies occur, and the top origin of those anomalies. You can access it by clicking on the **Flag** icon in the left navigation bar, then selecting the **Service Diagnosis Dashboard** menu item:\r\n\r\n ![](images/lab4-Picture68.png)\r\n\r\n","type":"Mdx","contentDigest":"39a9f672fa9c33b7cc1b8450bd863e6d","owner":"gatsby-plugin-mdx","counter":938},"frontmatter":{"title":"Reviewing Ingestion Results","description":"This lab will walk you through the steps to analyze and review the results of metric ingestion and machine learning"},"exports":{},"rawBody":"---\r\ntitle: Reviewing Ingestion Results\r\ndescription: This lab will walk you through the steps to analyze and review the results of metric ingestion and machine learning \r\n---\r\n\r\n<AnchorLinks>\r\n  <AnchorLink>5-1: Lab Introduction</AnchorLink>\r\n  <AnchorLink>5-2: Accessing the dashboard</AnchorLink>\r\n  <AnchorLink>5-3: The anomaly search view</AnchorLink>\r\n  <AnchorLink>5-4: Launching anomaly details</AnchorLink>\r\n  <AnchorLink>5-5: Viewing alarms in Event Manager</AnchorLink>\r\n  <AnchorLink>5-6: Metric search</AnchorLink>\r\n  <AnchorLink>5-7: Anomaly distribution dashboard</AnchorLink>\r\n</AnchorLinks>\r\n\r\n\r\n## 5-1: Lab Introduction\r\n\r\nIn our previous lab, we configured a Metric Manager topic to receive data, we built a data model based on CSV files, we deployed the model to our topic, and we ran extraction to allow the Metric Manager to ingest the data and learn the behavior. Now that we have run our data through the system for analysis and machine learning, we will review the results of the metric analysis.\r\n\r\n## 5-2: Accessing the dashboard\r\n\r\n\r\nTo view the results of the ingestion and analysis, open Firefox from the Desktop menu:\r\n\r\n ![](images/lab4-Picture49.png)\r\n\r\nLogin as ncoadmin / **<span style=\"color:green\"><LAB PASSWORD\\></span>**:\r\n\r\n ![](images/lab4-Picture50.png)\r\n\r\n (note, if you don't see the above login screen when opening up Firefox, click on the \"IBM Dashboard Application Services Hub\" bookmark in the bookmarks bar).\r\n\r\n## 5-3: The anomaly search view\r\n\r\n From the main Dashboard window, find the \"Snowflake\" icon in the left menu pane, click on it, and select \"Service Diagnosis Anomaly Search\":\r\n\r\n ![](images/lab4-Picture51.png)\r\n\r\n You will be presented with the anomaly search screen, which defaults to the latest anomalies that exist across all topics in the Metric Manager instance. Note that at the top you can do arbitrary date range searches by selecting a start date/time and an end/date time:\r\n\r\n ![](images/lab4-Picture52.png)\r\n\r\n You can also filter down the anomaly list by entering a resource name, with the UI doing a live search for resources as you type. Try this by typing in \"details\" in the \"resource\" box:\r\n\r\n ![](images/lab4-add1.png)\r\n\r\n Take note of how it auto-fills the resource information per the resources that are in the system. In this case, we have two resources:\r\n\r\n -\"details\" which would likely have \"node-level\", metrics (aggregate CPU utilization, for example).\r\n -\"details:DB\\_SAN1\" is a more specific component of the node \"details\", and indicates metrics for that particular sub-component (in this case, the disk DB\\_SAN1).\r\n\r\n You can also filter by metric name as well.\r\n\r\n Next, let's take a look at the anomalies that were generated. Click on the \"Show Current Anomalies\" icon. This will bring you back to the last set of anomalies that exist in the system:\r\n\r\n ![](images/lab4-add2.png)\r\n\r\n Notice the anomaly that indicates there was an incident on 3 metrics across two nodes. This is a grouping of related anomalies. Open up the tree view to expose all of the multi-level, grouped anomaly alerts:\r\n\r\n ![](images/lab4-Picture53.png)\r\n\r\n## 5-4: Launching anomaly details\r\n\r\n Next, find the anomaly that indicates **ResponseTime is Higher than expected** for the **productpage-v1** resource, and select **Launch** (may be in a different location that what is shown below):\r\n\r\n ![](images/lab4-Picture54.png)\r\n\r\n\r\n This will launch you into the anomaly diagnosis screen for the Response Time anomaly:\r\n\r\n ![](images/lab4-Picture55.png)\r\n\r\n Here, you can see the general pattern of the metric, and the training period leading up to our anomaly. By default, the robust bounds algorithm training period, which provides the high/low bounds of a given metric, is set to 28 days. You do have control over the training period for each of the algorithms employed in Metric Manager. In MAD, the default training period is 14 days, and we'll see how that differs in the MAD lab, where we catch each of the three spikes seen above in the 28-day training period.\r\n\r\n For this view, let's zoom into the latest time leading up to this anomaly. You can do this by hovering your mouse over the graph at a time previous to the red area… hold down your left mouse button, then drag right to capture the time period, then release the left mouse button. When you release the left mouse button, it will change the graph into a zoomed view:\r\n\r\n\r\n ![](images/lab4-Picture56.png)\r\n\r\n Here we can hover over the first box of the red area and see the exact time that this metric went out of its learned baseline. We see the anomalous area in red, with the actual values of the metrics continuing up.\r\n\r\n Note the light pink section of the box. That area indicates the \"grace period\" before we generate an alert to Watson AIOps Event Manager. By default, if any **three** of **six** consecutive data points of a resource/metric combination are found to be anomalous, then Metric Manager will generate an alert, and continue to generate those alerts for each consecutive anomalous data point. Anomalies that don't reach this threshold are still generated, but are **suppressed**. They can still be viewed in the anomaly search window by selecting \"Include suppressed alarms\" but these are not forwarded to the Event Manager. This behaviour is configurable by modifying the following pair of topic configuration items:\r\n\r\n`nofm.error.count.min.size: 3`  - the number of anomalies within...\r\n`nofm.error.count.window.size: 6`  - any X consecutive time intervals\r\n\r\nFor 5-minute interval data, using the default configuration, that means that the earliest we would see an anomaly alert is within 15 minutes of the resource/metric going anomalous (e.g. 3 consecutive anomalous periods), and the longest we might see an alert is 30 minutes (e.g. first data point is anomalous, followed by 3 normal data points, then followed by 2 more anomalous data points, as an example). The properties could both be set to 1, which would result in immediate alarms for all detected anomalies.\r\n\r\n\r\nAt the right-hand side of the graph, you can see a dotted line illustrating the general trend and forecast of the anomaly. \r\n\r\n At the bottom of this view, you can see a list of metrics, with Response time selected. Crucially, there are additional related metrics that are contributing to this Response time issue, and they are included as **Related Metrics**. In the bottom section of the window, add the related metrics to the view by clicking on the respective check boxes:\r\n\r\n ![](images/lab4-Picture57.png)\r\n\r\nThe graph will build out to include the related metrics, to give you a full picture of the situation.\r\n\r\nHover your mouse over the graph, and note that, for any point in time on the graph, you can see the metric names, values, and time:\r\n\r\n ![](images/lab4-Picture58.png)\r\n\r\nIn the \"Related Metrics\" section below, you see six columns: Metric, Resource, Anomalous, Actual/Expected Value, Info, and Baseline.\r\n\r\n The Metric and Resource columns indicate the metric name and the resource associated with that metric that is anomalous.\r\n\r\n The **Anomalous** icon tells you which algorithm has deemed this particular resource/metric combination anomalous. Using your mouse, hover over the **Anomalous** icon for the **ResponseTime** entry for the **productpage-v1** resource. Note that the hover text notes **ResponseTime has gone outside of its normal expected value**. This indicates that the Robust Bounds algorithm has deemed this metric to be anomalous. That fact is also evidenced by the graph, where we see the baseline of the metric, and when it went outside of the baseline.\r\n\r\n ![](images/lab4-Picture59.png)\r\n\r\n\r\n In contrast, hover over the **Anomalous** icon for the **TransactionsPerInterval** for the **details** resource. Note the hover text indicating that the **TransactionsPerInterval** metric for the details database has flat-lined … that is, where before it was varying, now it is no longer varying. This is an algorithm that identifies an unusual change in behaviour that would very likely describe a problematic situation. In this case, the number of database transactions have not been able to exceed 410, where normally we would see it around 2500.\r\n\r\n ![](images/lab4-Picture60.png)\r\n\r\n\r\n To the right of the Anomalous column is the **Actual/Expected Value**. Looking at the response time metric for the productpage-v1 application, we see its actual value is just over five times slower than expected for this time of day (6638/1290).\r\n\r\nThe **Info** column provides information with regards to why a metric has been included in the group of metrics. For example, hover over the blue information icon for the \"TransactionsPerInterval\" metric. It is indicating that there are two reasons why this metric was included:\r\n\r\n**When ResponseTime on productpage-v1 is anomalous, 100% of the time TransactionsPerInterval on details is also anomalous** They are very strongly correlated from an anomaly occurrence prespective.\r\n\r\n**This metric is causally related to the target metric you launched**. This indicates that the behavior of these two metrics (productpage-v1/ResponseTime, which is the metric we launched to, and details/TransactionsPerInterval) tend to act in concert.\r\n\r\n ![](images/lab4-Picture61.png)\r\n\r\n Looking at the overall related anomalies gives cross-silo visibility that would otherwise not be possible by only looking at the specific monitoring tools that are collecting this data. We have brought together performance data from our APM solution, which provides response time information for the productpage-v1 application. We have brought in database transaction information from our database monitoring tool. And we have brought in disk performance metrics from our infrastructure monitoring tool. In seeing these anomalies together, we can surmise that the disk saturation is limiting the number of database transactions, which is, in turn, causing a serious performance problem for the application.\r\n\r\n## 5-5: Viewing alarms in Event Manager\r\n\r\nNext, we will look at the alarms in Event Manager that were generated for these anomalies. Anomaly alerts are generated through the use of a stdin probe that is installed and configured during the installation of the Metric Manager. There are default views and filters that are also installed into DASH, and a pre-configure event list that shows metric related events. To access this event list, click on the flag icon in the left pane, then select **Detected Anomalies** :\r\n\r\n ![](images/lab4-Picture62.png)\r\n\r\n\r\n At the top of this list, you can see a number of alerts that indicate general progress of the topic, including when the topic was started, when the extractor was started/stopped, when we started receiving data, the amount of data received required for training, when the model training started, and when the model was produced:\r\n\r\n ![](images/lab4-Picture63.png)\r\n\r\nThe anomaly alerts are shown below, including \"summarized\" anomaly alerts. Right-click on the alert indicating \"Incident on 3 metrics across 2 nodes\", and select the \"ServiceDiagnosis\"\r\n\r\n ![](images/lab4-Picture64.png)\r\n\r\n Much like from the Anomaly Search window, you are launched into the anomaly graph:\r\n\r\n ![](images/lab4-Picture65.png)\r\n\r\nOne thing that's a little bit different than looking at the anomaly search, is that you must click on the \"View Child Alarms\" tool to see anomaly events that make up the the incident:\r\n\r\n ![](images/lab4-Picture66.png)\r\n\r\n..which brings us the child alarms. You may need to again run the tool to see sub-groups of metrics as well:\r\n\r\n ![](images/lab4-Picture67.png)\r\n\r\n## 5-6: Metric search\r\n\r\nYou can perform aribitrary searches for resource / metric combinations. \r\n\r\nClick on the \"Snowflake\" icon in the left menu, and select **Service Diagnosis Metric Search**.\r\n\r\n ![](images/lab4-add3.png)\r\n\r\nThe Metric Search screen appears, and here you can search for an arbitrary resource / metric combination by starting with the resource name (in this case search for **productpage-v1**), then the metric group (ApplicationPerformance), and finally the metric name (ResponseTime). When you have the metric of interest, click on \"Add\" to add the metric to the graph:\r\n\r\n ![](images/lab4-add4.png)\r\n\r\ngives way to...\r\n\r\n\r\n ![](images/lab4-add5.png)\r\n\r\n\r\nSearching for aribitrary metrics allows you to view the behaviour of metrics **without** them necessarily being anomalous, which can come in handy if you want to ensure that a given metric is being collected, and also in situations where someone needs evidence that a metric or set of metrics of interest are acting within their learned baselines.\r\n\r\n## 5-7: Anomaly distribution dashboard\r\n\r\nFinally, there is also an anomaly distribution dashboard that shows the time frame of anomalies received, the number of anomalies that occurred for each time frame in a bar graph, and top 10 anomalous nodes, resources, and metrics. Its more useful for large datasets to be able to identify the when, over time, the most anomalies occur, and the top origin of those anomalies. You can access it by clicking on the **Flag** icon in the left navigation bar, then selecting the **Service Diagnosis Dashboard** menu item:\r\n\r\n ![](images/lab4-Picture68.png)\r\n\r\n","fileAbsolutePath":"C:/Users/103537778/git/waiops-tech-jam/src/pages/tutorials/mm/mm-analyzing-results/index.mdx"}}},"staticQueryHashes":["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}